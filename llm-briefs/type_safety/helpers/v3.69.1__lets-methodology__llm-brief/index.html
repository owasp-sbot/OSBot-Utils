<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://owasp-sbot.github.io/OSBot-Utils/llm-briefs/type_safety/helpers/v3.69.1__lets-methodology__llm-brief/" />
      <link rel="shortcut icon" href="../../../../img/favicon.ico" />
    <title>LETS (Load, Extract, Transform, Save) - LLM Usage Brief - OSBot-Utils Documentation</title>
    <link rel="stylesheet" href="../../../../css/theme.css" />
    <link rel="stylesheet" href="../../../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "LETS (Load, Extract, Transform, Save) - LLM Usage Brief";
        var mkdocs_page_input_path = "llm-briefs/type_safety/helpers/v3.69.1__lets-methodology__llm-brief.md";
        var mkdocs_page_url = "/OSBot-Utils/llm-briefs/type_safety/helpers/v3.69.1__lets-methodology__llm-brief/";
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../../../.." class="icon icon-home"> OSBot-Utils Documentation
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../..">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Code</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" >OSBot Utils</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" >Helpers</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../../../code/osbot_utils/helpers/flows/osbot-utils-flow-system-documentation/">Flows</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Development</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../../dev/Python-code-formatting-guidelines/">Coding Guidelines</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Type Safety</a>
    <ul>
                <li class="toctree-l2"><a class="" href="../../../../dev/type_safe/python-type-safety-frameworks-compared.md">Frameworks Compared</a>
                </li>
                <li class="toctree-l2"><a class="" href="../../../../dev/type_safe/type-safe-technical-documentation.md">Technical Documentation</a>
                </li>
    </ul>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../..">OSBot-Utils Documentation</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../../.." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">LETS (Load, Extract, Transform, Save) - LLM Usage Brief</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="lets-load-extract-transform-save-llm-usage-brief">LETS (Load, Extract, Transform, Save) - LLM Usage Brief<a class="headerlink" href="#lets-load-extract-transform-save-llm-usage-brief" title="Permanent link">&para;</a></h1>
<p><strong>Version</strong>: v1.1.0<br />
<strong>Purpose</strong>: Guide for LLMs and developers on implementing deterministic, debuggable data pipelines<br />
<strong>Author</strong>: Dinis Cruz &amp; ChatGPT Deep Research<br />
<strong>Related Projects</strong>: MyFeeds.ai, MGraph-AI, The Cyber Boardroom<br />
<strong>Prerequisites</strong>: Type_Safe core guide, Safe Primitives reference</p>
<hr />
<h2 id="what-is-lets">What is LETS?<a class="headerlink" href="#what-is-lets" title="Permanent link">&para;</a></h2>
<p><strong>LETS (Load, Extract, Transform, Save) is a data pipeline architecture built on Type_Safe foundations that enables strongly-typed data to flow seamlessly through every stage with automatic serialization/deserialization, full traceability, and deterministic behavior.</strong></p>
<p>The key insight: LETS isn't just about saving files—it's about <strong>Type_Safe schemas in → Type_Safe schemas out</strong> at every stage, with the file system providing persistence, versioning, and debuggability.</p>
<h3 id="the-problem-it-solves">The Problem It Solves<a class="headerlink" href="#the-problem-it-solves" title="Permanent link">&para;</a></h3>
<p>Traditional ETL/ELT pipelines suffer from critical shortcomings:</p>
<pre><code>┌──────────────────────────────────────────────────────────────────────┐
│                    Traditional ETL Pipeline                          │
│  ────────────────────────────────────────────────────────────────── │
│                                                                      │
│  Source → [BLACK BOX TRANSFORMATIONS] → Destination                  │
│                    ↑                                                 │
│           • No intermediate visibility                               │
│           • Errors surface only at the end                           │
│           • Can't replay or debug individual steps                   │
│           • No version history of data states                        │
│           • Poor reproducibility                                     │
│           • Type information lost between stages                     │
│           • Silent data corruption from untyped dicts                │
└──────────────────────────────────────────────────────────────────────┘
</code></pre>
<p><strong>With LETS + Type_Safe:</strong></p>
<pre><code>┌──────────────────────────────────────────────────────────────────────┐
│                  LETS Pipeline (Type_Safe Foundation)                │
│  ────────────────────────────────────────────────────────────────── │
│                                                                      │
│  Schema__Raw ──► Schema__Structured ──► Schema__Enriched            │
│       │                  │                      │                    │
│       ▼                  ▼                      ▼                    │
│   .json()            .json()                .json()                  │
│       │                  │                      │                    │
│       ▼                  ▼                      ▼                    │
│  raw.json ────► structured.json ────► enriched.json                 │
│       │                  │                      │                    │
│       ▼                  ▼                      ▼                    │
│  .from_json()        .from_json()           .from_json()            │
│       │                  │                      │                    │
│       ▼                  ▼                      ▼                    │
│  Schema__Raw      Schema__Structured     Schema__Enriched           │
│                                                                      │
│  ✓ Full type safety at every stage                                  │
│  ✓ Automatic serialization preserves types                          │
│  ✓ Runtime validation catches errors immediately                    │
│  ✓ Versioned, inspectable, replayable                               │
└──────────────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="design-philosophy">Design Philosophy<a class="headerlink" href="#design-philosophy" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Type_Safe schemas at every boundary</strong> — Not raw dicts, but validated Type_Safe classes</li>
<li><strong>Safe Primitives, not raw primitives</strong> — <code>Safe_Str__Email</code>, not <code>str</code>; <code>Safe_UInt__Port</code>, not <code>int</code></li>
<li><strong>Every stage saves its output</strong> — No hidden in-memory states; all data is externalized</li>
<li><strong>File systems as databases</strong> — S3, local disk, or Git repositories become your data stores</li>
<li><strong>Determinism over convenience</strong> — Same input + same code = same output, always</li>
<li><strong>Minimum viable propagation</strong> — Process only what changed, not everything</li>
</ol>
<hr />
<h2 id="the-type_safe-foundation">The Type_Safe Foundation<a class="headerlink" href="#the-type_safe-foundation" title="Permanent link">&para;</a></h2>
<p><strong>This is what makes LETS truly powerful.</strong> Type_Safe provides the foundation for strongly-typed data flow:</p>
<h3 id="why-type_safe-is-essential-for-lets">Why Type_Safe is Essential for LETS<a class="headerlink" href="#why-type_safe-is-essential-for-lets" title="Permanent link">&para;</a></h3>
<pre><code class="language-python"># ❌ TRADITIONAL PIPELINE: Raw dicts, no validation, silent corruption
def transform(data: dict) -&gt; dict:
    return {&quot;entities&quot;: extract_entities(data[&quot;content&quot;])}  # What's in &quot;entities&quot;? Who knows!

# ✓ LETS PIPELINE: Type_Safe schemas, validated at every boundary
def transform(data: Schema__Article) -&gt; Schema__Article__Enriched:
    entities = extract_entities(data.content)
    return Schema__Article__Enriched(
        article_id=data.article_id,
        entities=entities  # Type-checked!
    )
</code></pre>
<h3 id="core-components">Core Components<a class="headerlink" href="#core-components" title="Permanent link">&para;</a></h3>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│                    Type_Safe Foundation for LETS                    │
│ ─────────────────────────────────────────────────────────────────── │
│                                                                     │
│  ┌─────────────────────┐    ┌─────────────────────┐                │
│  │   Safe Primitives   │    │   Type_Safe Classes │                │
│  │ ─────────────────── │    │ ─────────────────── │                │
│  │ Safe_Str__Email     │    │ Schema__Article     │                │
│  │ Safe_UInt__Port     │    │ Schema__Entity      │                │
│  │ Safe_Float__Money   │    │ Schema__Graph       │                │
│  │ Safe_Id             │    │                     │                │
│  └─────────────────────┘    └─────────────────────┘                │
│            │                          │                             │
│            └──────────┬───────────────┘                             │
│                       ▼                                             │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │              Type_Safe Collections                           │   │
│  │ ─────────────────────────────────────────────────────────── │   │
│  │ Dict__Entities__By_Id    List__Article__Tags                │   │
│  │ Set__Permission__Ids     Tuple__Coordinates                 │   │
│  └─────────────────────────────────────────────────────────────┘   │
│                       │                                             │
│                       ▼                                             │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │              Automatic Serialization                         │   │
│  │ ─────────────────────────────────────────────────────────── │   │
│  │ schema.json() ──► JSON file ──► Schema.from_json()          │   │
│  │ Types preserved through entire round-trip                    │   │
│  └─────────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="defining-pipeline-schemas">Defining Pipeline Schemas<a class="headerlink" href="#defining-pipeline-schemas" title="Permanent link">&para;</a></h3>
<p><strong>CRITICAL: Use Safe Primitives, NOT raw primitives!</strong></p>
<pre><code class="language-python">from osbot_utils.type_safe.Type_Safe import Type_Safe
from osbot_utils.type_safe.primitives.core.Safe_Str                           import Safe_Str
from osbot_utils.type_safe.primitives.core.Safe_UInt                          import Safe_UInt
from osbot_utils.type_safe.primitives.domains.identifiers.Safe_Id             import Safe_Id
from osbot_utils.type_safe.primitives.domains.identifiers.Timestamp_Now       import Timestamp_Now
from osbot_utils.type_safe.primitives.domains.identifiers.Random_Guid         import Random_Guid
from osbot_utils.type_safe.primitives.domains.web.safe_str.Safe_Str__Url      import Safe_Str__Url
from osbot_utils.type_safe.primitives.domains.llm.safe_str.Safe_Str__LLM__Prompt import Safe_Str__LLM__Prompt

# ❌ NEVER: Raw primitives allow invalid data
class Bad__Article(Type_Safe):
    id       : str                      # Can be empty, SQL injection, any length
    title    : str                      # No validation
    url      : str                      # Could be &quot;not-a-url&quot;
    priority : int                      # Could be -999

# ✓ ALWAYS: Safe primitives validate at construction
class Schema__Article(Type_Safe):
    article_id  : Safe_Id               # Sanitized, length-limited identifier
    title       : Safe_Str              # Validated string
    url         : Safe_Str__Url         # Must be valid URL with http(s)://
    priority    : Safe_UInt             # Unsigned integer (≥0)
    fetched_at  : Timestamp_Now         # Auto-generates current timestamp
</code></pre>
<h3 id="domain-specific-primitives-for-pipelines">Domain-Specific Primitives for Pipelines<a class="headerlink" href="#domain-specific-primitives-for-pipelines" title="Permanent link">&para;</a></h3>
<p>Choose the right primitive for your domain:</p>
<pre><code class="language-python"># Identifiers
from osbot_utils.type_safe.primitives.domains.identifiers.Safe_Id         import Safe_Id
from osbot_utils.type_safe.primitives.domains.identifiers.Random_Guid     import Random_Guid
from osbot_utils.type_safe.primitives.domains.identifiers.Obj_Id          import Obj_Id
from osbot_utils.type_safe.primitives.domains.identifiers.Timestamp_Now   import Timestamp_Now

# LLM-specific (essential for LETS + GenAI)
from osbot_utils.type_safe.primitives.domains.llm.safe_str.Safe_Str__LLM__Prompt    import Safe_Str__LLM__Prompt
from osbot_utils.type_safe.primitives.domains.llm.safe_str.Safe_Str__LLM__Model_Id  import Safe_Str__LLM__Model_Id
from osbot_utils.type_safe.primitives.domains.llm.safe_float.Safe_Float__LLM__Temperature import Safe_Float__LLM__Temperature

# Web/HTTP
from osbot_utils.type_safe.primitives.domains.web.safe_str.Safe_Str__Url     import Safe_Str__Url
from osbot_utils.type_safe.primitives.domains.web.safe_str.Safe_Str__Email   import Safe_Str__Email

# Files
from osbot_utils.type_safe.primitives.domains.files.safe_str.Safe_Str__File__Path import Safe_Str__File__Path

# Numerical
from osbot_utils.type_safe.primitives.domains.numerical.safe_float.Safe_Float__Money import Safe_Float__Money

# Cryptography (for content hashing in caches)
from osbot_utils.type_safe.primitives.domains.cryptography.safe_str.Safe_Str__Hash import Safe_Str__Hash
</code></pre>
<h3 id="type_safe-collections-for-pipeline-data">Type_Safe Collections for Pipeline Data<a class="headerlink" href="#type_safe-collections-for-pipeline-data" title="Permanent link">&para;</a></h3>
<p>Use typed collections, not raw <code>dict</code> or <code>list</code>:</p>
<pre><code class="language-python">from osbot_utils.type_safe.type_safe_core.collections.Type_Safe__Dict import Type_Safe__Dict
from osbot_utils.type_safe.type_safe_core.collections.Type_Safe__List import Type_Safe__List

# Define reusable collection types (use Dict__, List__, Set__ prefixes)
class Dict__Entities__By_Id(Type_Safe__Dict):
    expected_key_type   = Safe_Id
    expected_value_type = Schema__Entity

class List__Article__Tags(Type_Safe__List):
    expected_type = Safe_Str

class Dict__Articles__By_Hash(Type_Safe__Dict):
    expected_key_type   = Safe_Str__Hash          # Content hash as key
    expected_value_type = Schema__Article

# Use in schemas
class Schema__Entity_Graph(Type_Safe):
    graph_id   : Safe_Id
    entities   : Dict__Entities__By_Id            # Type-safe collection
    tags       : List__Article__Tags
    created_at : Timestamp_Now
</code></pre>
<h3 id="automatic-serializationdeserialization">Automatic Serialization/Deserialization<a class="headerlink" href="#automatic-serializationdeserialization" title="Permanent link">&para;</a></h3>
<p>The magic of LETS: <strong>types are preserved through JSON round-trips</strong>:</p>
<pre><code class="language-python"># Stage 1: Create typed data
article = Schema__Article(
    article_id=&quot;article-001&quot;,
    title=&quot;Breaking News&quot;,
    url=&quot;https://example.com/news&quot;,
    priority=5
)

# Serialize to JSON file (for LETS Save step)
json_data = article.json()                        # Returns dict
# Save to file: {&quot;article_id&quot;: &quot;article-001&quot;, &quot;title&quot;: &quot;Breaking News&quot;, ...}

# Deserialize back (for LETS Load step of next stage)
loaded = Schema__Article.from_json(json_data)

# Types are fully preserved!
assert isinstance(loaded.article_id, Safe_Id)
assert isinstance(loaded.url, Safe_Str__Url)
assert isinstance(loaded.priority, Safe_UInt)
</code></pre>
<h3 id="creating-custom-pipeline-primitives">Creating Custom Pipeline Primitives<a class="headerlink" href="#creating-custom-pipeline-primitives" title="Permanent link">&para;</a></h3>
<p>Define domain-specific types for your pipeline:</p>
<pre><code class="language-python">import re
from osbot_utils.type_safe.primitives.core.Safe_Str  import Safe_Str
from osbot_utils.type_safe.primitives.core.Safe_UInt import Safe_UInt

# Custom identifier for your domain
class Article_Id(Safe_Str):
    max_length = 64
    regex      = re.compile(r'[^a-z0-9\-]')       # Only lowercase, numbers, hyphens
    allow_empty = False

class Persona_Id(Safe_Str):
    max_length = 32
    regex      = re.compile(r'[^a-z0-9_]')

# Custom numeric type with constraints  
class Relevance_Score(Safe_UInt):
    min_value = 0
    max_value = 100                               # 0-100 score

# Use in pipeline schemas
class Schema__Article__Match(Type_Safe):
    article_id     : Article_Id
    persona_id     : Persona_Id
    relevance      : Relevance_Score
    matched_topics : List__Article__Tags
</code></pre>
<h3 id="the-four-stages">The Four Stages<a class="headerlink" href="#the-four-stages" title="Permanent link">&para;</a></h3>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│                                                                     │
│   LOAD → EXTRACT → TRANSFORM → SAVE                                 │
│                                                                     │
│   Load: Fetch raw data from external sources                        │
│         → SAVE to raw storage layer                                 │
│                                                                     │
│   Extract: Parse/structure the raw data                             │
│         → SAVE structured representation                            │
│                                                                     │
│   Transform: Apply business logic, ML, enrichment                   │
│         → SAVE transformed output                                   │
│                                                                     │
│   Save: Finalize and version the outputs                            │
│         → Commit to permanent storage with timestamp                │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<hr />
<h2 id="quick-start">Quick Start<a class="headerlink" href="#quick-start" title="Permanent link">&para;</a></h2>
<h3 id="the-four-stages_1">The Four Stages<a class="headerlink" href="#the-four-stages_1" title="Permanent link">&para;</a></h3>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│                                                                     │
│   LOAD → EXTRACT → TRANSFORM → SAVE                                 │
│                                                                     │
│   Load: Fetch raw data, deserialize to Schema__Raw                  │
│         → schema.json() → save to raw storage layer                 │
│                                                                     │
│   Extract: Parse/structure into Schema__Structured                  │
│         → schema.json() → save structured representation            │
│                                                                     │
│   Transform: Enrich to Schema__Enriched (ML, LLM, etc.)            │
│         → schema.json() → save transformed output                   │
│                                                                     │
│   Save: Finalize with dual-save (latest + temporal)                 │
│         → Commit to permanent storage with timestamp                │
│                                                                     │
│   Every stage: Type_Safe in → Type_Safe out → .json() → file        │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
</code></pre>
<h3 id="basic-implementation-pattern">Basic Implementation Pattern<a class="headerlink" href="#basic-implementation-pattern" title="Permanent link">&para;</a></h3>
<pre><code class="language-python">from osbot_utils.type_safe.Type_Safe                                      import Type_Safe
from osbot_utils.type_safe.primitives.core.Safe_Str                       import Safe_Str
from osbot_utils.type_safe.primitives.domains.identifiers.Safe_Id         import Safe_Id
from osbot_utils.type_safe.primitives.domains.identifiers.Timestamp_Now   import Timestamp_Now
from osbot_utils.type_safe.primitives.domains.identifiers.Random_Guid     import Random_Guid
from osbot_utils.type_safe.primitives.domains.web.safe_str.Safe_Str__Url  import Safe_Str__Url
from osbot_utils.utils.Files import file_create, file_contents, folder_create
from osbot_utils.utils.Json  import json_dumps, json_loads
from typing import List

# ═══════════════════════════════════════════════════════════════════════
# STEP 1: Define Type_Safe schemas for each pipeline stage
# ═══════════════════════════════════════════════════════════════════════

class Schema__Raw__Feed(Type_Safe):
    &quot;&quot;&quot;Schema for Load stage output&quot;&quot;&quot;
    feed_id    : Safe_Id
    url        : Safe_Str__Url
    content    : Safe_Str                         # Raw RSS/XML content
    fetched_at : Timestamp_Now

class Schema__Article(Type_Safe):
    &quot;&quot;&quot;Schema for Extract stage output&quot;&quot;&quot;
    article_id   : Safe_Id
    title        : Safe_Str
    author       : Safe_Str
    url          : Safe_Str__Url
    published_at : Safe_Str                       # ISO timestamp
    content      : Safe_Str

class Schema__Article__Extracted(Type_Safe):
    &quot;&quot;&quot;Schema for Extract stage - all articles from a feed&quot;&quot;&quot;
    feed_id    : Safe_Id
    source_url : Safe_Str__Url
    articles   : List[Schema__Article]            # Auto-converts to Type_Safe__List
    created_at : Timestamp_Now

class Schema__Entity(Type_Safe):
    &quot;&quot;&quot;Entity extracted by LLM&quot;&quot;&quot;
    name        : Safe_Str
    entity_type : Safe_Str                        # &quot;person&quot;, &quot;organization&quot;, &quot;technology&quot;
    confidence  : float                           # 0.0-1.0

class Schema__Article__Enriched(Type_Safe):
    &quot;&quot;&quot;Schema for Transform stage output&quot;&quot;&quot;
    article_id      : Safe_Id
    original        : Schema__Article             # Nested Type_Safe preserved!
    entities        : List[Schema__Entity]
    llm_model       : Safe_Str
    transform_version: Safe_Str

# ═══════════════════════════════════════════════════════════════════════
# STEP 2: LETS Pipeline with dual-save pattern
# ═══════════════════════════════════════════════════════════════════════

class LETS_Pipeline:
    &quot;&quot;&quot;Base pattern for a LETS pipeline with Type_Safe schemas&quot;&quot;&quot;

    def __init__(self, base_path: str):
        self.base_path     = base_path
        self.latest_path   = f&quot;{base_path}/latest&quot;
        self.temporal_path = f&quot;{base_path}/temporal&quot;

    def save_schema(self, schema: Type_Safe, filename: str, timestamp: str) -&gt; tuple:
        &quot;&quot;&quot;Save Type_Safe schema to both /latest and timestamped location&quot;&quot;&quot;
        json_data = json_dumps(schema.json())     # Type_Safe → dict → JSON string

        # Save to latest (for quick access)
        folder_create(self.latest_path)
        latest_file = f&quot;{self.latest_path}/{filename}&quot;
        file_create(latest_file, json_data)

        # Save to temporal (for versioning/audit)
        temporal_folder = f&quot;{self.temporal_path}/{timestamp}&quot;
        folder_create(temporal_folder)
        temporal_file = f&quot;{temporal_folder}/{filename}&quot;
        file_create(temporal_file, json_data)

        return latest_file, temporal_file

    def load_schema(self, schema_class: type, filepath: str) -&gt; Type_Safe:
        &quot;&quot;&quot;Load JSON file back into Type_Safe schema&quot;&quot;&quot;
        json_str = file_contents(filepath)
        json_data = json_loads(json_str)
        return schema_class.from_json(json_data)  # Full type restoration!

# ═══════════════════════════════════════════════════════════════════════
# STEP 3: Pipeline stages using Type_Safe
# ═══════════════════════════════════════════════════════════════════════

def load_stage(url: str, pipeline: LETS_Pipeline, timestamp: str) -&gt; str:
    &quot;&quot;&quot;LOAD: Fetch and save raw data as Type_Safe schema&quot;&quot;&quot;
    import requests
    response = requests.get(url)

    raw_feed = Schema__Raw__Feed(
        feed_id    = f&quot;feed-{Random_Guid()}&quot;,
        url        = url,
        content    = response.text,
        # fetched_at auto-generates via Timestamp_Now
    )

    latest, temporal = pipeline.save_schema(raw_feed, &quot;raw-feed.json&quot;, timestamp)
    return latest

def extract_stage(raw_path: str, pipeline: LETS_Pipeline, timestamp: str) -&gt; str:
    &quot;&quot;&quot;EXTRACT: Parse raw data into structured Type_Safe schemas&quot;&quot;&quot;
    raw_feed = pipeline.load_schema(Schema__Raw__Feed, raw_path)

    # Parse RSS/XML into structured articles (your parsing logic)
    articles = parse_feed_to_articles(raw_feed.content)  

    extracted = Schema__Article__Extracted(
        feed_id    = raw_feed.feed_id,
        source_url = raw_feed.url,
        articles   = articles,
        # created_at auto-generates
    )

    latest, temporal = pipeline.save_schema(extracted, &quot;articles.json&quot;, timestamp)
    return latest

def transform_stage(articles_path: str, pipeline: LETS_Pipeline, timestamp: str) -&gt; str:
    &quot;&quot;&quot;TRANSFORM: Enrich with LLM using structured Type_Safe output&quot;&quot;&quot;
    extracted = pipeline.load_schema(Schema__Article__Extracted, articles_path)

    enriched_articles = []
    for article in extracted.articles:
        # LLM returns Type_Safe schema, NOT free-form text
        entities = extract_entities_with_llm(article)

        enriched = Schema__Article__Enriched(
            article_id       = article.article_id,
            original         = article,                # Nested schema preserved
            entities         = entities,
            llm_model        = &quot;gpt-4&quot;,
            transform_version= &quot;1.0.0&quot;,
        )
        enriched_articles.append(enriched)

    # Save each enriched article
    for enriched in enriched_articles:
        pipeline.save_schema(
            enriched, 
            f&quot;enriched-{enriched.article_id}.json&quot;, 
            timestamp
        )

    return f&quot;{pipeline.latest_path}/enriched&quot;

---

## Core Concepts

### 1. Dual-Save Pattern (Latest + Temporal)

Every piece of data is saved in two locations:

</code></pre>
<p>data/
├── latest/                          # Current state (overwritten)
│   ├── feed-timeline.json
│   └── articles/
│       └── article-001.json
│
└── temporal/                        # Historical versions (append-only)
    └── 2025/03/26/11/              # Year/Month/Day/Hour
        ├── feed-timeline.json
        └── articles/
            └── article-001.json</p>
<pre><code>
**Why both?**
- `latest/` — Fast access to current state; easy to query &quot;what's the current X?&quot;
- `temporal/` — Full audit trail; enables time-travel debugging and rollback

### 2. Structured Outputs with Type_Safe (Especially for LLMs)

**The key to deterministic LLM pipelines is Type_Safe schemas, not raw JSON:**

```python
from osbot_utils.type_safe.Type_Safe                                        import Type_Safe
from osbot_utils.type_safe.primitives.core.Safe_Str                         import Safe_Str
from osbot_utils.type_safe.primitives.core.Safe_Float                       import Safe_Float
from osbot_utils.type_safe.primitives.domains.identifiers.Safe_Id           import Safe_Id
from osbot_utils.type_safe.type_safe_core.collections.Type_Safe__List       import Type_Safe__List
from typing import List

# ═══════════════════════════════════════════════════════════════════════
# Define Type_Safe schemas for LLM outputs
# ═══════════════════════════════════════════════════════════════════════

class Schema__Entity(Type_Safe):
    &quot;&quot;&quot;Single entity extracted by LLM&quot;&quot;&quot;
    name        : Safe_Str
    entity_type : Safe_Str              # &quot;person&quot;, &quot;organization&quot;, &quot;technology&quot;
    confidence  : Safe_Float

class Schema__Relationship(Type_Safe):
    &quot;&quot;&quot;Relationship between entities&quot;&quot;&quot;
    source      : Safe_Str
    verb        : Safe_Str              # &quot;uses&quot;, &quot;founded&quot;, &quot;works_at&quot;
    target      : Safe_Str
    confidence  : Safe_Float

class Schema__Article__Graph(Type_Safe):
    &quot;&quot;&quot;Complete entity graph for an article - LLM output schema&quot;&quot;&quot;
    article_id    : Safe_Id
    entities      : List[Schema__Entity]
    relationships : List[Schema__Relationship]

# ═══════════════════════════════════════════════════════════════════════
# Use Type_Safe schema with LLM (e.g., OpenAI structured outputs)
# ═══════════════════════════════════════════════════════════════════════

def extract_entities_with_llm(article: Schema__Article) -&gt; Schema__Article__Graph:
    &quot;&quot;&quot;LLM fills the Type_Safe schema, not free-form text&quot;&quot;&quot;

    # Generate JSON schema from Type_Safe class
    schema_description = &quot;&quot;&quot;
    Extract entities and relationships. Return JSON:
    {
      &quot;article_id&quot;: &quot;...&quot;,
      &quot;entities&quot;: [{&quot;name&quot;: &quot;...&quot;, &quot;entity_type&quot;: &quot;...&quot;, &quot;confidence&quot;: 0.0-1.0}],
      &quot;relationships&quot;: [{&quot;source&quot;: &quot;...&quot;, &quot;verb&quot;: &quot;...&quot;, &quot;target&quot;: &quot;...&quot;, &quot;confidence&quot;: 0.0-1.0}]
    }
    &quot;&quot;&quot;

    response = openai.chat.completions.create(
        model=&quot;gpt-4&quot;,
        temperature=0,                            # Determinism!
        response_format={&quot;type&quot;: &quot;json_object&quot;},  # Structured output
        messages=[{
            &quot;role&quot;: &quot;system&quot;,
            &quot;content&quot;: schema_description
        }, {
            &quot;role&quot;: &quot;user&quot;, 
            &quot;content&quot;: f&quot;Article: {article.title}\n\n{article.content}&quot;
        }]
    )

    # Parse LLM JSON into Type_Safe schema - VALIDATES automatically!
    json_data = json.loads(response.choices[0].message.content)
    json_data['article_id'] = str(article.article_id)  # Ensure ID matches

    return Schema__Article__Graph.from_json(json_data)
    # ↑ This validates ALL fields match the schema
    # If LLM returns malformed data, it fails IMMEDIATELY, not downstream

# ═══════════════════════════════════════════════════════════════════════
# The power: Type_Safe → JSON → Type_Safe with full validation
# ═══════════════════════════════════════════════════════════════════════

# Save to file (LETS Save step)
graph = extract_entities_with_llm(article)
json_data = graph.json()                          # Type_Safe → dict
file_create(&quot;graph.json&quot;, json_dumps(json_data))

# Load from file (LETS Load step of next stage)  
loaded_data = json_loads(file_contents(&quot;graph.json&quot;))
restored_graph = Schema__Article__Graph.from_json(loaded_data)

# Types are FULLY PRESERVED
assert isinstance(restored_graph.article_id, Safe_Id)
assert isinstance(restored_graph.entities[0], Schema__Entity)
assert isinstance(restored_graph.entities[0].name, Safe_Str)
</code></pre>
<p><strong>Benefits of Type_Safe LLM outputs:</strong>
- <strong>Deterministic-ish behavior</strong> — Same schema = comparable, validatable results
- <strong>Immediate validation</strong> — Malformed LLM output fails at <code>from_json()</code>, not deep in pipeline
- <strong>Traceable reasoning</strong> — Each field can be audited
- <strong>Testable</strong> — Compare Type_Safe objects in CI with <code>==</code>
- <strong>Type preservation</strong> — Loaded data has exact same types as when saved</p>
<h3 id="3-provenance-chain">3. Provenance Chain<a class="headerlink" href="#3-provenance-chain" title="Permanent link">&para;</a></h3>
<p>Every output contains references to its inputs, creating an auditable chain:</p>
<pre><code class="language-json">{
  &quot;output_id&quot;: &quot;enriched-article-001&quot;,
  &quot;created_at&quot;: &quot;2025-03-26T11:30:00Z&quot;,
  &quot;source_files&quot;: [
    &quot;temporal/2025/03/26/11/raw-feed.json&quot;,
    &quot;temporal/2025/03/26/11/articles/article-001.json&quot;
  ],
  &quot;transform_version&quot;: &quot;v1.2.0&quot;,
  &quot;data&quot;: { ... }
}
</code></pre>
<p>This enables answering: "Why was this article recommended?" by tracing the chain.</p>
<h3 id="4-memory-fs-pattern-ephemeral-compute-persistent-storage">4. Memory-FS Pattern (Ephemeral Compute + Persistent Storage)<a class="headerlink" href="#4-memory-fs-pattern-ephemeral-compute-persistent-storage" title="Permanent link">&para;</a></h3>
<p>For serverless/lambda architectures:</p>
<pre><code>┌──────────────────────────────────────────────────────────────────────┐
│                      Memory-FS Pattern                               │
│  ────────────────────────────────────────────────────────────────── │
│                                                                      │
│  ┌─────────────┐    Read     ┌─────────────────────────────┐        │
│  │  In-Memory  │◄────────────│   Persistent Storage        │        │
│  │   Graph     │             │   (S3, Local Disk, SQLite)  │        │
│  │  (fast ops) │────────────►│   (durable state)           │        │
│  └─────────────┘    Write    └─────────────────────────────┘        │
│                                                                      │
│  • Memory provides speed for computation                             │
│  • Filesystem provides persistence and versioning                    │
│  • Each Lambda invocation is stateless                               │
│  • State loaded from files, saved back after processing              │
└──────────────────────────────────────────────────────────────────────┘
</code></pre>
<hr />
<h2 id="integration-with-osbot-utils-mgraph-ai">Integration with OSBot-Utils / MGraph-AI<a class="headerlink" href="#integration-with-osbot-utils-mgraph-ai" title="Permanent link">&para;</a></h2>
<h3 id="using-cache-service-for-lets-storage">Using Cache Service for LETS Storage<a class="headerlink" href="#using-cache-service-for-lets-storage" title="Permanent link">&para;</a></h3>
<p>The MGraph-AI Cache Service provides ideal storage strategies for LETS pipelines:</p>
<pre><code class="language-python">from mgraph_ai_cache_service.clients.Client__Cache_Service import Client__Cache_Service

class LETS_Cache_Pipeline:
    def __init__(self, base_url: str, namespace: str = &quot;pipeline&quot;):
        self.client = Client__Cache_Service(base_url)
        self.namespace = namespace

    def save_stage_output(self, stage: str, data: dict, cache_key: str):
        &quot;&quot;&quot;Save using temporal_latest strategy for dual-save pattern&quot;&quot;&quot;
        return self.client.store_key_based_json(
            data=data,
            path=f&quot;{stage}/{cache_key}&quot;,
            namespace=self.namespace
        )

    def load_stage_input(self, cache_id: str):
        &quot;&quot;&quot;Load previous stage's output&quot;&quot;&quot;
        return self.client.retrieve_json(cache_id, namespace=self.namespace)
</code></pre>
<p><strong>Recommended strategies:</strong>
- <code>temporal_latest</code> — Best for LETS (provides both latest pointer and historical versions)
- <code>temporal_versioned</code> — When you need explicit version numbers
- <code>key_based</code> — For semantic, human-readable paths</p>
<h3 id="using-mgraph-ai-for-knowledge-graphs">Using MGraph-AI for Knowledge Graphs<a class="headerlink" href="#using-mgraph-ai-for-knowledge-graphs" title="Permanent link">&para;</a></h3>
<pre><code class="language-python">from mgraph_ai.mgraph.MGraph import MGraph

class LETS_Graph_Pipeline:
    def __init__(self, storage_path: str):
        self.graph = MGraph()
        self.storage_path = storage_path

    def load_graph(self, graph_file: str):
        &quot;&quot;&quot;Load graph state from JSON file&quot;&quot;&quot;
        self.graph.from_json_file(f&quot;{self.storage_path}/{graph_file}&quot;)
        return self

    def transform_and_save(self, transform_fn, output_file: str, timestamp: str):
        &quot;&quot;&quot;Apply transformation and save with LETS pattern&quot;&quot;&quot;
        transform_fn(self.graph)

        # Save to latest
        self.graph.to_json_file(f&quot;{self.storage_path}/latest/{output_file}&quot;)

        # Save to temporal
        self.graph.to_json_file(f&quot;{self.storage_path}/{timestamp}/{output_file}&quot;)

        return self
</code></pre>
<hr />
<h2 id="real-world-example-myfeedsai-pipeline">Real-World Example: MyFeeds.ai Pipeline<a class="headerlink" href="#real-world-example-myfeedsai-pipeline" title="Permanent link">&para;</a></h2>
<p>MyFeeds.ai demonstrates LETS with an LLM-powered content personalization pipeline:</p>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│                    MyFeeds.ai LETS Pipeline                         │
└─────────────────────────────────────────────────────────────────────┘

Stage 1: LOAD
├── Input: RSS feed URL
├── Output: raw-feed.json
└── Saved to: /feeds/{feed_id}/latest/raw-feed.json
              /feeds/{feed_id}/2025/03/26/11/raw-feed.json

Stage 2: EXTRACT  
├── Input: raw-feed.json
├── Output: articles/*.json (one per article)
└── Saved to: /feeds/{feed_id}/latest/articles/
              /feeds/{feed_id}/2025/03/26/11/articles/

Stage 3: TRANSFORM (Entity Extraction)
├── Input: article.json
├── LLM Call: Extract entities with structured schema
├── Output: article-entities.json
└── Saved to: /articles/{article_id}/latest/entities.json
              /articles/{article_id}/2025/03/26/11/entities.json

Stage 4: TRANSFORM (Persona Graph)
├── Input: persona-profile.json
├── LLM Call: Generate interest graph with structured schema
├── Output: persona-graph.json
└── Saved to: /personas/{persona_id}/latest/graph.json

Stage 5: TRANSFORM (Relevance Mapping)
├── Input: article-entities.json + persona-graph.json
├── Output: relevance-map.json (which entities overlap)
└── Saved to: /matches/{persona_id}/{article_id}/relevance.json

Stage 6: TRANSFORM (Summary Generation)
├── Input: article.json + relevance-map.json
├── LLM Call: Generate personalized summary
├── Output: summary.json (with provenance links)
└── Saved to: /summaries/{persona_id}/{article_id}/summary.json

RESULT: Full provenance chain explains WHY article was recommended
</code></pre>
<p><strong>Key implementation details:</strong>
- Each stage is a separate FastAPI endpoint (Flow 1, Flow 2, etc.)
- Each endpoint reads files from S3, processes, saves back to S3
- File naming includes article IDs for traceability
- All LLM outputs use structured JSON schemas</p>
<hr />
<h2 id="the-five-principles-of-lets">The Five Principles of LETS<a class="headerlink" href="#the-five-principles-of-lets" title="Permanent link">&para;</a></h2>
<h3 id="1-ephemerality-of-compute">1. Ephemerality (of Compute)<a class="headerlink" href="#1-ephemerality-of-compute" title="Permanent link">&para;</a></h3>
<pre><code class="language-python"># ✅ LETS: Stateless function, all state in files
def process_article(article_id: str, storage: Storage):
    article = storage.load(f&quot;articles/{article_id}.json&quot;)
    enriched = enrich_with_llm(article)
    storage.save(f&quot;enriched/{article_id}.json&quot;, enriched)
    # Function ends, no state retained

# ❌ Anti-pattern: State held in memory between calls
class StatefulProcessor:
    def __init__(self):
        self.processed_articles = {}  # State lives here - BAD
</code></pre>
<h3 id="2-traceability-provenance">2. Traceability (Provenance)<a class="headerlink" href="#2-traceability-provenance" title="Permanent link">&para;</a></h3>
<pre><code class="language-python"># ✅ LETS: Every output knows its lineage
output = {
    &quot;id&quot;: &quot;output-001&quot;,
    &quot;source_files&quot;: [&quot;input-a.json&quot;, &quot;input-b.json&quot;],
    &quot;transform&quot;: &quot;merge_and_enrich&quot;,
    &quot;transform_version&quot;: &quot;1.2.0&quot;,
    &quot;created_at&quot;: &quot;2025-03-26T11:30:00Z&quot;,
    &quot;data&quot;: { ... }
}

# ❌ Anti-pattern: Orphan data with no lineage
output = { &quot;data&quot;: { ... } }  # Where did this come from?
</code></pre>
<h3 id="3-determinism-reproducibility">3. Determinism (Reproducibility)<a class="headerlink" href="#3-determinism-reproducibility" title="Permanent link">&para;</a></h3>
<pre><code class="language-python"># ✅ LETS: Same inputs → same outputs
def transform(input_file: str, seed: int = 42) -&gt; dict:
    data = load(input_file)
    random.seed(seed)  # Control randomness
    result = process(data)
    return result

# ❌ Anti-pattern: Uncontrolled non-determinism  
def transform(input_file: str) -&gt; dict:
    data = load(input_file)
    result = process_with_random_sampling(data)  # Different every time!
    return result
</code></pre>
<h3 id="4-modularity-single-responsibility">4. Modularity (Single Responsibility)<a class="headerlink" href="#4-modularity-single-responsibility" title="Permanent link">&para;</a></h3>
<pre><code class="language-python"># ✅ LETS: Each stage does one thing
def load_feed(url: str) -&gt; dict: ...
def extract_articles(feed: dict) -&gt; list: ...
def transform_article(article: dict) -&gt; dict: ...

# ❌ Anti-pattern: Monolithic pipeline
def do_everything(url: str) -&gt; dict:
    feed = requests.get(url).json()
    articles = parse_xml(feed)
    for a in articles:
        entities = extract_entities(a)
        graph = build_graph(entities)
        summary = generate_summary(graph)
    return summaries  # No intermediate state saved!
</code></pre>
<h3 id="5-minimum-viable-propagation-efficiency">5. Minimum Viable Propagation (Efficiency)<a class="headerlink" href="#5-minimum-viable-propagation-efficiency" title="Permanent link">&para;</a></h3>
<pre><code class="language-python"># ✅ LETS: Process only what changed
def incremental_process(new_article_ids: list[str], storage: Storage):
    for article_id in new_article_ids:
        if not storage.exists(f&quot;enriched/{article_id}.json&quot;):
            article = storage.load(f&quot;articles/{article_id}.json&quot;)
            enriched = enrich(article)
            storage.save(f&quot;enriched/{article_id}.json&quot;, enriched)

# ❌ Anti-pattern: Reprocess everything
def full_reprocess(storage: Storage):
    all_articles = storage.list(&quot;articles/&quot;)
    for article_file in all_articles:  # Processes 10,000 articles even if 1 is new
        article = storage.load(article_file)
        enriched = enrich(article)
        storage.save(f&quot;enriched/{article_file}&quot;, enriched)
</code></pre>
<hr />
<h2 id="best-practices">Best Practices<a class="headerlink" href="#best-practices" title="Permanent link">&para;</a></h2>
<h3 id="do-use-safe-primitives-never-raw-primitives">DO: Use Safe Primitives, NEVER Raw Primitives<a class="headerlink" href="#do-use-safe-primitives-never-raw-primitives" title="Permanent link">&para;</a></h3>
<pre><code class="language-python">from osbot_utils.type_safe.primitives.core.Safe_Str                       import Safe_Str
from osbot_utils.type_safe.primitives.core.Safe_UInt                      import Safe_UInt
from osbot_utils.type_safe.primitives.domains.identifiers.Safe_Id         import Safe_Id
from osbot_utils.type_safe.primitives.domains.web.safe_str.Safe_Str__Url  import Safe_Str__Url

# ❌ NEVER: Raw primitives in pipeline schemas
class Bad__Pipeline__Stage(Type_Safe):
    id      : str                         # Any string, no validation
    count   : int                         # Can be negative!
    url     : str                         # Could be &quot;not-a-url&quot;

# ✓ ALWAYS: Safe primitives with domain validation
class Schema__Pipeline__Stage(Type_Safe):
    stage_id : Safe_Id                    # Sanitized identifier
    count    : Safe_UInt                  # Unsigned (≥0)
    url      : Safe_Str__Url              # Validated URL
</code></pre>
<h3 id="do-define-type_safe-schemas-for-every-stage">DO: Define Type_Safe Schemas for Every Stage<a class="headerlink" href="#do-define-type_safe-schemas-for-every-stage" title="Permanent link">&para;</a></h3>
<pre><code class="language-python"># ✓ Good - explicit schemas for each stage
class Schema__Raw__Feed(Type_Safe):       # Load stage output
    feed_id   : Safe_Id
    content   : Safe_Str

class Schema__Parsed__Articles(Type_Safe): # Extract stage output
    feed_id   : Safe_Id
    articles  : List[Schema__Article]

class Schema__Enriched__Articles(Type_Safe): # Transform stage output
    feed_id   : Safe_Id
    articles  : List[Schema__Article__Enriched]

# ❌ Bad - raw dicts with no schema
def transform(data: dict) -&gt; dict:        # What's in data? Who knows!
    return {&quot;result&quot;: process(data)}
</code></pre>
<h3 id="do-use-type_safe-collections-dict__-list__-set__-subclasses">DO: Use Type_Safe Collections (Dict__, List__, Set__ subclasses)<a class="headerlink" href="#do-use-type_safe-collections-dict__-list__-set__-subclasses" title="Permanent link">&para;</a></h3>
<pre><code class="language-python">from osbot_utils.type_safe.type_safe_core.collections.Type_Safe__Dict import Type_Safe__Dict
from osbot_utils.type_safe.type_safe_core.collections.Type_Safe__List import Type_Safe__List

# ✓ Good - named, reusable typed collections
class Dict__Articles__By_Id(Type_Safe__Dict):
    expected_key_type   = Safe_Id
    expected_value_type = Schema__Article

class List__Entity__Names(Type_Safe__List):
    expected_type = Safe_Str

class Schema__Article__Index(Type_Safe):
    by_id   : Dict__Articles__By_Id       # Type-safe Dict
    tags    : List__Entity__Names         # Type-safe List

# ❌ Bad - raw dict loses type info at runtime
class Bad__Article__Index(Type_Safe):
    by_id   : dict                        # No type enforcement!
    tags    : list                        # Anything can go in here
</code></pre>
<h3 id="do-schemas-are-pure-data-no-methods">DO: Schemas are Pure Data - NO Methods<a class="headerlink" href="#do-schemas-are-pure-data-no-methods" title="Permanent link">&para;</a></h3>
<pre><code class="language-python"># ✓ CORRECT: Schema is ONLY data
class Schema__Article__Graph(Type_Safe):
    article_id : Safe_Id
    entities   : List[Schema__Entity]
    edges      : List[Schema__Relationship]

# ✓ CORRECT: Logic in separate helper class
class Article__Graph__Utils(Type_Safe):
    @type_safe
    def find_connected_entities(self, graph: Schema__Article__Graph, 
                                      entity_name: Safe_Str) -&gt; List[Schema__Entity]:
        # Logic here
        pass

# ❌ WRONG: Methods mixed with data
class Bad__Article__Graph(Type_Safe):
    article_id : Safe_Id
    entities   : List[Schema__Entity]

    def find_connected(self, name: str):   # NO! Move to helper class
        pass
</code></pre>
<h3 id="do-save-type_safe-schemas-after-every-stage">DO: Save Type_Safe Schemas After Every Stage<a class="headerlink" href="#do-save-type_safe-schemas-after-every-stage" title="Permanent link">&para;</a></h3>
<pre><code class="language-python"># ✓ Good - checkpoint after each stage with Type_Safe
raw_feed    = load_feed(url)              # Returns Schema__Raw__Feed
pipeline.save_schema(raw_feed, &quot;raw.json&quot;, timestamp)

parsed      = parse_feed(raw_feed)        # Returns Schema__Parsed__Articles
pipeline.save_schema(parsed, &quot;parsed.json&quot;, timestamp)

enriched    = enrich_with_llm(parsed)     # Returns Schema__Enriched__Articles
pipeline.save_schema(enriched, &quot;enriched.json&quot;, timestamp)

# ❌ Bad - only save final output
raw = load_feed(url)
parsed = parse_feed(raw)
enriched = enrich_with_llm(parsed)
save(enriched, &quot;final.json&quot;)              # Lost intermediate Type_Safe states!
</code></pre>
<h3 id="do-use-json-and-from_json-for-serialization">DO: Use .json() and .from_json() for Serialization<a class="headerlink" href="#do-use-json-and-from_json-for-serialization" title="Permanent link">&para;</a></h3>
<pre><code class="language-python"># ✓ Good - Type_Safe serialization methods
schema = Schema__Article(article_id=&quot;001&quot;, title=&quot;News&quot;)
json_dict = schema.json()                 # → dict with all fields
file_create(&quot;article.json&quot;, json_dumps(json_dict))

# Load back with full type restoration
loaded_dict = json_loads(file_contents(&quot;article.json&quot;))
restored = Schema__Article.from_json(loaded_dict)
assert isinstance(restored.article_id, Safe_Id)  # Types preserved!

# ❌ Bad - manual dict construction loses types
bad_dict = {&quot;article_id&quot;: schema.article_id, &quot;title&quot;: schema.title}
# Lost type information!
</code></pre>
<h3 id="dont-use-free-form-llm-outputs">DON'T: Use Free-Form LLM Outputs<a class="headerlink" href="#dont-use-free-form-llm-outputs" title="Permanent link">&para;</a></h3>
<pre><code class="language-python"># ❌ Bad - unstructured LLM output
response = llm.complete(&quot;Extract entities from this article&quot;)
entities = response.text                  # Free-form string - unparseable!

# ✓ Good - Type_Safe schema enforced
response = llm.complete(
    &quot;Extract entities. Return JSON: {entities: [{name, type, confidence}]}&quot;
)
json_data = json_loads(response.text)
entities = Schema__Entity__List.from_json({&quot;items&quot;: json_data[&quot;entities&quot;]})
# Validation happens automatically!
</code></pre>
<h3 id="dont-hold-state-between-pipeline-runs">DON'T: Hold State Between Pipeline Runs<a class="headerlink" href="#dont-hold-state-between-pipeline-runs" title="Permanent link">&para;</a></h3>
<pre><code class="language-python"># ❌ Bad - global state
_cache = {}

def process(article_id: Safe_Id) -&gt; Schema__Article__Enriched:
    if article_id in _cache:              # Memory state - lost on restart!
        return _cache[article_id]
    result = expensive_compute(article_id)
    _cache[article_id] = result
    return result

# ✓ Good - file-based state with Type_Safe
def process(article_id: Safe_Id, pipeline: LETS_Pipeline) -&gt; Schema__Article__Enriched:
    cache_path = f&quot;cache/{article_id}.json&quot;
    if file_exists(cache_path):
        return pipeline.load_schema(Schema__Article__Enriched, cache_path)
    result = expensive_compute(article_id)
    pipeline.save_schema(result, f&quot;{article_id}.json&quot;, get_timestamp())
    return result
</code></pre>
<hr />
<h2 id="testing-lets-pipelines">Testing LETS Pipelines<a class="headerlink" href="#testing-lets-pipelines" title="Permanent link">&para;</a></h2>
<h3 id="unit-test-each-stage-independently">Unit Test Each Stage Independently<a class="headerlink" href="#unit-test-each-stage-independently" title="Permanent link">&para;</a></h3>
<pre><code class="language-python">import pytest
from osbot_utils.utils.Files import file_create, file_contents, temp_folder

class Test_Extract_Stage:

    def test_extract_produces_expected_structure(self):
        # Arrange: Create known input
        test_dir = temp_folder()
        raw_input = {&quot;content&quot;: &quot;&lt;rss&gt;...&lt;/rss&gt;&quot;}
        file_create(f&quot;{test_dir}/raw.json&quot;, json.dumps(raw_input))

        # Act: Run extract stage
        output_path = extract_stage(f&quot;{test_dir}/raw.json&quot;, f&quot;{test_dir}/output.json&quot;)

        # Assert: Output matches expected schema
        output = json.loads(file_contents(output_path))
        assert &quot;articles&quot; in output
        assert &quot;source&quot; in output
        assert output[&quot;source&quot;] == f&quot;{test_dir}/raw.json&quot;

    def test_extract_is_deterministic(self):
        # Same input should produce identical output
        test_dir = temp_folder()
        raw_input = {&quot;content&quot;: &quot;&lt;rss&gt;...&lt;/rss&gt;&quot;}
        file_create(f&quot;{test_dir}/raw.json&quot;, json.dumps(raw_input))

        output1 = extract_stage(f&quot;{test_dir}/raw.json&quot;, f&quot;{test_dir}/output1.json&quot;)
        output2 = extract_stage(f&quot;{test_dir}/raw.json&quot;, f&quot;{test_dir}/output2.json&quot;)

        assert file_contents(output1) == file_contents(output2)
</code></pre>
<h3 id="golden-file-testing">Golden File Testing<a class="headerlink" href="#golden-file-testing" title="Permanent link">&para;</a></h3>
<pre><code class="language-python">class Test_Transform_Golden_Files:
    &quot;&quot;&quot;Compare outputs against known-good golden files&quot;&quot;&quot;

    def test_entity_extraction_matches_golden(self):
        # Load golden input/output pair
        input_data = load_golden_input(&quot;test_article_001.json&quot;)
        expected_output = load_golden_output(&quot;test_article_001_entities.json&quot;)

        # Run transform
        actual_output = extract_entities(input_data)

        # Compare (ignoring timestamps)
        assert_json_equal(actual_output, expected_output, ignore=[&quot;created_at&quot;])
</code></pre>
<h3 id="cicd-integration">CI/CD Integration<a class="headerlink" href="#cicd-integration" title="Permanent link">&para;</a></h3>
<pre><code class="language-yaml"># .github/workflows/pipeline-test.yml
name: LETS Pipeline Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Run Stage Unit Tests
        run: pytest tests/unit/

      - name: Run Integration Tests (with test data)
        run: pytest tests/integration/ --test-data-dir=tests/fixtures/

      - name: Verify Determinism
        run: |
          python run_pipeline.py --input tests/fixtures/sample.json --output /tmp/run1/
          python run_pipeline.py --input tests/fixtures/sample.json --output /tmp/run2/
          diff -r /tmp/run1/ /tmp/run2/  # Should be identical
</code></pre>
<hr />
<h2 id="troubleshooting">Troubleshooting<a class="headerlink" href="#troubleshooting" title="Permanent link">&para;</a></h2>
<h3 id="problem-llm-outputs-vary-between-runs">Problem: LLM Outputs Vary Between Runs<a class="headerlink" href="#problem-llm-outputs-vary-between-runs" title="Permanent link">&para;</a></h3>
<p><strong>Cause</strong>: LLM temperature &gt; 0 or no structured output schema</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-python"># Set temperature to 0 for maximum determinism
response = openai.chat.completions.create(
    model=&quot;gpt-4&quot;,
    temperature=0,  # Deterministic
    response_format={&quot;type&quot;: &quot;json_object&quot;},  # Structured output
    messages=[...]
)
</code></pre>
<h3 id="problem-cant-trace-why-a-result-was-generated">Problem: Can't Trace Why a Result Was Generated<a class="headerlink" href="#problem-cant-trace-why-a-result-was-generated" title="Permanent link">&para;</a></h3>
<p><strong>Cause</strong>: Missing provenance metadata</p>
<p><strong>Solution</strong>: Always include source references:</p>
<pre><code class="language-python">output = {
    &quot;source_files&quot;: [input_file_1, input_file_2],
    &quot;transform_version&quot;: __version__,
    &quot;created_at&quot;: datetime.utcnow().isoformat(),
    &quot;data&quot;: result
}
</code></pre>
<h3 id="problem-pipeline-crashes-mid-way-lost-all-progress">Problem: Pipeline Crashes Mid-Way, Lost All Progress<a class="headerlink" href="#problem-pipeline-crashes-mid-way-lost-all-progress" title="Permanent link">&para;</a></h3>
<p><strong>Cause</strong>: Not saving after each stage</p>
<p><strong>Solution</strong>: Implement stage checkpointing:</p>
<pre><code class="language-python">def run_pipeline(input_path: str, resume_from: str = None):
    stages = [&quot;load&quot;, &quot;extract&quot;, &quot;transform&quot;, &quot;save&quot;]
    start_idx = stages.index(resume_from) if resume_from else 0

    for stage in stages[start_idx:]:
        input_file = get_stage_input(stage)
        output_file = run_stage(stage, input_file)
        save_checkpoint(stage, output_file)  # Can resume from here
</code></pre>
<h3 id="problem-storage-growing-too-fast">Problem: Storage Growing Too Fast<a class="headerlink" href="#problem-storage-growing-too-fast" title="Permanent link">&para;</a></h3>
<p><strong>Cause</strong>: Temporal versioning without cleanup</p>
<p><strong>Solution</strong>: Implement retention policy:</p>
<pre><code class="language-python">def cleanup_old_versions(storage: Storage, retention_days: int = 30):
    &quot;&quot;&quot;Delete temporal versions older than retention period&quot;&quot;&quot;
    cutoff = datetime.utcnow() - timedelta(days=retention_days)
    for version_path in storage.list(&quot;temporal/&quot;):
        version_date = parse_date_from_path(version_path)
        if version_date &lt; cutoff:
            storage.delete(version_path)
</code></pre>
<hr />
<h2 id="summary-checklist">Summary Checklist<a class="headerlink" href="#summary-checklist" title="Permanent link">&para;</a></h2>
<p>When implementing a LETS pipeline:</p>
<h3 id="type_safe-foundation-critical">Type_Safe Foundation (CRITICAL)<a class="headerlink" href="#type_safe-foundation-critical" title="Permanent link">&para;</a></h3>
<ul>
<li>[ ] <strong>Use Safe Primitives</strong> — <code>Safe_Id</code>, <code>Safe_Str__Url</code>, <code>Safe_UInt</code> — NEVER raw <code>str</code>, <code>int</code>, <code>float</code></li>
<li>[ ] <strong>Define Type_Safe schemas</strong> for every pipeline stage input/output</li>
<li>[ ] <strong>Use Type_Safe Collections</strong> — <code>Dict__</code>, <code>List__</code>, <code>Set__</code> subclasses for typed containers</li>
<li>[ ] <strong>Schemas are PURE DATA</strong> — No methods in schema classes, logic goes in helper classes</li>
<li>[ ] <strong>Use <code>.json()</code> and <code>.from_json()</code></strong> for serialization/deserialization</li>
<li>[ ] <strong>Validate at boundaries</strong> — <code>Schema.from_json()</code> validates automatically</li>
</ul>
<h3 id="pipeline-structure">Pipeline Structure<a class="headerlink" href="#pipeline-structure" title="Permanent link">&para;</a></h3>
<ul>
<li>[ ] <strong>Load Stage</strong>: Fetch raw data → <code>Schema__Raw</code> → save immediately</li>
<li>[ ] <strong>Extract Stage</strong>: Parse/structure → <code>Schema__Structured</code> → save</li>
<li>[ ] <strong>Transform Stage</strong>: Enrich/ML/LLM → <code>Schema__Enriched</code> → save</li>
<li>[ ] <strong>Save Stage</strong>: Version outputs with dual-save (latest + temporal)</li>
</ul>
<h3 id="lets-principles">LETS Principles<a class="headerlink" href="#lets-principles" title="Permanent link">&para;</a></h3>
<ul>
<li>[ ] <strong>Dual-save pattern</strong>: Save to both <code>/latest/</code> and <code>/temporal/{timestamp}/</code></li>
<li>[ ] <strong>Provenance metadata</strong>: Include source files, version, timestamp in outputs</li>
<li>[ ] <strong>Ephemerality of compute</strong>: No in-memory state between pipeline runs</li>
<li>[ ] <strong>Human-readable paths</strong>: Semantic file paths, not opaque hashes</li>
<li>[ ] <strong>Minimum viable propagation</strong>: Process only what changed</li>
</ul>
<h3 id="llm-integration">LLM Integration<a class="headerlink" href="#llm-integration" title="Permanent link">&para;</a></h3>
<ul>
<li>[ ] <strong>Type_Safe schemas for LLM outputs</strong> — Define the schema the LLM must fill</li>
<li>[ ] <strong>Use <code>temperature=0</code></strong> for maximum determinism</li>
<li>[ ] <strong>Validate immediately</strong> — <code>Schema.from_json(llm_output)</code> catches malformed output</li>
<li>[ ] <strong>No free-form text</strong> — LLM returns structured JSON matching Type_Safe schema</li>
</ul>
<h3 id="testing-ci">Testing &amp; CI<a class="headerlink" href="#testing-ci" title="Permanent link">&para;</a></h3>
<ul>
<li>[ ] Each stage independently runnable and testable</li>
<li>[ ] Golden file tests with Type_Safe comparisons</li>
<li>[ ] Verify determinism (same input → same output)</li>
<li>[ ] Implement retention policy for temporal versions</li>
</ul>
<h3 id="related-documentation">Related Documentation<a class="headerlink" href="#related-documentation" title="Permanent link">&para;</a></h3>
<p>When implementing LETS pipelines, request these guides:
- <code>v3.63.4__for_llms__type_safe.md</code> — Core Type_Safe patterns
- <code>v3.28.0__osbot-utils-safe-primitives__reference-guide.md</code> — Safe primitive catalog
- <code>v3.63.3__for_llms__type_safe__collections__subclassing_guide.md</code> — Typed collections
- <code>v0.5.68__cache-service__llm-brief.md</code> — Storage backend for LETS</p>
<hr />
<h2 id="further-reading">Further Reading<a class="headerlink" href="#further-reading" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Type_Safe Core Guide</strong>: Runtime type enforcement, serialization, Safe primitives</li>
<li><strong>Safe Primitives Reference</strong>: 100+ domain-specific types (LLM, Web, Files, etc.)</li>
<li><strong>Type_Safe Collections Guide</strong>: Creating Dict__, List__, Set__ subclasses</li>
<li><strong>MyFeeds.ai Architecture</strong>: https://mvp.myfeeds.ai</li>
<li><strong>MGraph-AI Documentation</strong>: Memory-first graph database for LETS storage</li>
<li><strong>The Cyber Boardroom</strong>: Serverless GenAI using LETS patterns</li>
<li><strong>Cache Service</strong>: Flexible storage backend supporting LETS strategies</li>
</ul>
              
            </div>
          </div><footer>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
    
  </span>
</div>
    <script src="../../../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../../../..";</script>
    <script src="../../../../js/theme_extra.js"></script>
    <script src="../../../../js/theme.js"></script>
      <script src="../../../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
