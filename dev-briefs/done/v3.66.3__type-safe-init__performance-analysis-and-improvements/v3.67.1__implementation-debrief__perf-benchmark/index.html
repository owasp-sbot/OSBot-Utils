<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://owasp-sbot.github.io/OSBot-Utils/dev-briefs/done/v3.66.3__type-safe-init__performance-analysis-and-improvements/v3.67.1__implementation-debrief__perf-benchmark/" />
      <link rel="shortcut icon" href="../../../../img/favicon.ico" />
    <title>Perf_Benchmark v4 - Technical Implementation Debrief - OSBot-Utils Documentation</title>
    <link rel="stylesheet" href="../../../../css/theme.css" />
    <link rel="stylesheet" href="../../../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Perf_Benchmark v4 - Technical Implementation Debrief";
        var mkdocs_page_input_path = "dev-briefs/done/v3.66.3__type-safe-init__performance-analysis-and-improvements/v3.67.1__implementation-debrief__perf-benchmark.md";
        var mkdocs_page_url = "/OSBot-Utils/dev-briefs/done/v3.66.3__type-safe-init__performance-analysis-and-improvements/v3.67.1__implementation-debrief__perf-benchmark/";
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../../../.." class="icon icon-home"> OSBot-Utils Documentation
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../..">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Code</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" >OSBot Utils</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" >Helpers</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../../../code/osbot_utils/helpers/flows/osbot-utils-flow-system-documentation/">Flows</a>
                </li>
    </ul>
                </li>
    </ul>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Development</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../../dev/Python-code-formatting-guidelines/">Coding Guidelines</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Type Safety</a>
    <ul>
                <li class="toctree-l2"><a class="" href="../../../../dev/type_safe/python-type-safety-frameworks-compared.md">Frameworks Compared</a>
                </li>
                <li class="toctree-l2"><a class="" href="../../../../dev/type_safe/type-safe-technical-documentation.md">Technical Documentation</a>
                </li>
    </ul>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../..">OSBot-Utils Documentation</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../../.." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Perf_Benchmark v4 - Technical Implementation Debrief</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="perf_benchmark-v4-technical-implementation-debrief">Perf_Benchmark v4 - Technical Implementation Debrief<a class="headerlink" href="#perf_benchmark-v4-technical-implementation-debrief" title="Permanent link">&para;</a></h1>
<h2 id="executive-summary">Executive Summary<a class="headerlink" href="#executive-summary" title="Permanent link">&para;</a></h2>
<p>This document details the design, implementation, and lessons learned from building the Perf_Benchmark v4 system - a type-safe, schema-driven performance benchmarking framework for Python. The system provides comprehensive timing, comparison, hypothesis testing, and multi-format export capabilities while maintaining strict type safety throughout.</p>
<p><strong>Key Metrics:</strong>
- 300+ passing tests
- 140ms total test execution time
- 22 core implementation files
- 19 test files covering all components
- Zero use of raw <code>str</code> or generic <code>Safe_Str</code> in final implementation</p>
<hr />
<h2 id="1-what-we-achieved">1. What We Achieved<a class="headerlink" href="#1-what-we-achieved" title="Permanent link">&para;</a></h2>
<h3 id="11-core-components">1.1 Core Components<a class="headerlink" href="#11-core-components" title="Permanent link">&para;</a></h3>
<h4 id="perf_benchmark__timing">Perf_Benchmark__Timing<a class="headerlink" href="#perf_benchmark__timing" title="Permanent link">&para;</a></h4>
<p>The central benchmarking engine that wraps <code>Performance_Measure__Session</code> with structured result capture.</p>
<pre><code>┌─────────────────────────────────────────────────────────────┐
│                  Perf_Benchmark__Timing                      │
├─────────────────────────────────────────────────────────────┤
│ config   : Schema__Perf_Benchmark__Timing__Config           │
│ results  : Dict__Benchmark_Results      (summaries)         │
│ sessions : Dict__Benchmark_Sessions     (full measurements) │
├─────────────────────────────────────────────────────────────┤
│ benchmark(id, target, threshold) → Schema__Perf__Result     │
│ reporter() → Perf_Benchmark__Timing__Reporter               │
└─────────────────────────────────────────────────────────────┘
</code></pre>
<p><strong>Key Features:</strong>
- Context manager support (<code>with Perf_Benchmark__Timing() as timing:</code>)
- Automatic session lifecycle management
- Dual storage: lightweight summaries + full measurement data
- Configurable thresholds with assertions
- Standard time constants (<code>time_100_ns</code>, <code>time_1_kns</code>, etc.)</p>
<h4 id="perf_benchmark__diff">Perf_Benchmark__Diff<a class="headerlink" href="#perf_benchmark__diff" title="Permanent link">&para;</a></h4>
<p>Multi-session comparison engine for tracking performance evolution over time.</p>
<pre><code>┌─────────────────────────────────────────────────────────────┐
│                   Perf_Benchmark__Diff                       │
├─────────────────────────────────────────────────────────────┤
│ sessions : List__Benchmark_Sessions                         │
├─────────────────────────────────────────────────────────────┤
│ load_session(filepath) → self                               │
│ load_folder(folder_path) → self                             │
│ compare_two(a, b) → Schema__Perf__Comparison__Two           │
│ compare_all() → Schema__Perf__Evolution                     │
│ statistics() → Schema__Perf__Statistics                     │
└─────────────────────────────────────────────────────────────┘
</code></pre>
<p><strong>Key Features:</strong>
- Returns structured schemas instead of formatted strings
- Status enums for error handling (<code>ERROR_NO_SESSIONS</code>, <code>ERROR_INSUFFICIENT_SESSIONS</code>)
- Trend detection with 5-level classification
- Clean separation of calculation vs presentation</p>
<h4 id="perf_benchmark__hypothesis">Perf_Benchmark__Hypothesis<a class="headerlink" href="#perf_benchmark__hypothesis" title="Permanent link">&para;</a></h4>
<p>Statistical hypothesis testing for performance assertions.</p>
<pre><code>┌─────────────────────────────────────────────────────────────┐
│                Perf_Benchmark__Hypothesis                    │
├─────────────────────────────────────────────────────────────┤
│ timing    : Perf_Benchmark__Timing                          │
│ baseline  : Dict__Benchmark_Results                         │
│ tolerance : Safe_Float (default 10%)                        │
├─────────────────────────────────────────────────────────────┤
│ set_baseline_from_results()                                 │
│ test_no_regression() → Schema__Perf__Hypothesis__Result     │
│ test_improvement(threshold) → ...                           │
│ test_within_tolerance(expected) → ...                       │
└─────────────────────────────────────────────────────────────┘
</code></pre>
<h4 id="export-system">Export System<a class="headerlink" href="#export-system" title="Permanent link">&para;</a></h4>
<p>Pluggable export architecture treating "print" as a type of export.</p>
<pre><code>Perf_Benchmark__Export (base)
    ├── Perf_Benchmark__Export__Text   (Print_Table formatting)
    ├── Perf_Benchmark__Export__HTML   (Chart.js visualizations)
    └── Perf_Benchmark__Export__JSON   (Schema serialization)
</code></pre>
<p>Each exporter implements:
- <code>export_comparison(Schema__Perf__Comparison__Two) → str</code>
- <code>export_evolution(Schema__Perf__Evolution) → str</code>
- <code>export_statistics(Schema__Perf__Statistics) → str</code></p>
<h3 id="12-schema-architecture">1.2 Schema Architecture<a class="headerlink" href="#12-schema-architecture" title="Permanent link">&para;</a></h3>
<h4 id="comparison-schemas">Comparison Schemas<a class="headerlink" href="#comparison-schemas" title="Permanent link">&para;</a></h4>
<pre><code class="language-python">Schema__Perf__Benchmark__Comparison     # Single benchmark diff
    benchmark_id   : Safe_Str__Benchmark_Id
    name           : Safe_Str__Benchmark__Title
    score_a        : Safe_UInt
    score_b        : Safe_UInt
    change_percent : Safe_Float__Percentage_Change
    trend          : Enum__Benchmark__Trend

Schema__Perf__Comparison__Two           # Two-session comparison result
    status      : Enum__Comparison__Status
    error       : Safe_Str__Benchmark__Description
    title_a     : Safe_Str__Benchmark__Title
    title_b     : Safe_Str__Benchmark__Title
    comparisons : List__Benchmark_Comparisons
    timestamp   : Timestamp_Now

Schema__Perf__Evolution                 # Multi-session evolution
    status        : Enum__Comparison__Status
    session_count : Safe_UInt
    titles        : List__Titles
    evolutions    : List__Benchmark_Evolutions
    timestamp     : Timestamp_Now

Schema__Perf__Statistics                # Summary statistics
    status            : Enum__Comparison__Status
    session_count     : Safe_UInt
    benchmark_count   : Safe_UInt
    improvement_count : Safe_UInt
    regression_count  : Safe_UInt
    avg_improvement   : Safe_Float
    avg_regression    : Safe_Float
    best_improvement  : Optional[Schema__Perf__Benchmark__Comparison]
    worst_regression  : Optional[Schema__Perf__Benchmark__Comparison]
</code></pre>
<h4 id="enums">Enums<a class="headerlink" href="#enums" title="Permanent link">&para;</a></h4>
<pre><code class="language-python">Enum__Comparison__Status
    SUCCESS
    ERROR_NO_SESSIONS
    ERROR_INSUFFICIENT_SESSIONS
    ERROR_NO_COMMON_BENCHMARKS

Enum__Benchmark__Trend
    STRONG_IMPROVEMENT   # &gt; 10%  ▼▼▼
    IMPROVEMENT          # 0-10%  ▼
    UNCHANGED            # 0%     ─
    REGRESSION           # 0-10%  ▲
    STRONG_REGRESSION    # &gt; 10%  ▲▲▲

Enum__Time_Unit
    NANOSECONDS
    MICROSECONDS
    MILLISECONDS
    SECONDS

Enum__Hypothesis__Status
    PASSED
    FAILED
    INCONCLUSIVE
</code></pre>
<h4 id="type-safe-collections">Type-Safe Collections<a class="headerlink" href="#type-safe-collections" title="Permanent link">&para;</a></h4>
<pre><code class="language-python">List__Benchmark_Sessions      # [Schema__Perf__Benchmark__Session, ...]
List__Benchmark_Comparisons   # [Schema__Perf__Benchmark__Comparison, ...]
List__Benchmark_Evolutions    # [Schema__Perf__Benchmark__Evolution, ...]
List__Titles                  # [Safe_Str__Benchmark__Title, ...]
List__Scores                  # [Safe_UInt, ...]

Dict__Benchmark_Results       # {Safe_Str__Benchmark_Id → Schema__Perf__Benchmark__Result}
Dict__Benchmark_Sessions      # {Safe_Str__Benchmark_Id → Performance_Measure__Session}
Dict__Benchmark__Legend       # {Safe_Str__Benchmark__Section → Safe_Str__Benchmark__Title}
</code></pre>
<h3 id="13-safe_str-primitive-hierarchy">1.3 Safe_Str Primitive Hierarchy<a class="headerlink" href="#13-safe_str-primitive-hierarchy" title="Permanent link">&para;</a></h3>
<pre><code>Safe_Str (base)
├── Safe_Str__Benchmark_Id              # &quot;A_01__test_name&quot; (max 100, alphanumeric + _)
├── Safe_Str__Benchmark__Section        # &quot;A&quot;, &quot;B&quot; (max 50, identifier-style)
├── Safe_Str__Benchmark__Index          # &quot;01&quot;, &quot;02&quot; (max 10, alphanumeric + _)
├── Safe_Str__Benchmark__Title          # Display names (max 200)
├── Safe_Str__Benchmark__Description    # Multi-paragraph technical content (max 4096)
├── Safe_Str__File__Path                # File system paths
├── Safe_Str__Text                      # General text output (max 1MB)
├── Safe_Str__Markdown                  # Markdown documents (max 1MB)
├── Safe_Str__Html                      # HTML documents (max 10MB)
├── Safe_Str__Javascript                # JavaScript code (max 1MB)
└── Safe_Str__Time_Formatted            # &quot;1,000 ns&quot;, &quot;1.5 µs&quot; (max 50)
</code></pre>
<h3 id="14-numeric-primitives">1.4 Numeric Primitives<a class="headerlink" href="#14-numeric-primitives" title="Permanent link">&para;</a></h3>
<pre><code class="language-python">Safe_Float__Percentage_Change           # Percentage deltas (-1M to +1M, 2 decimal places)
    min_value      = -1_000_000.0       # Up to 10,000× regression
    max_value      =  1_000_000.0       # Up to 10,000× improvement
    decimal_places = 2
    round_output   = True
</code></pre>
<hr />
<h2 id="2-how-we-did-it">2. How We Did It<a class="headerlink" href="#2-how-we-did-it" title="Permanent link">&para;</a></h2>
<h3 id="21-iterative-design-process">2.1 Iterative Design Process<a class="headerlink" href="#21-iterative-design-process" title="Permanent link">&para;</a></h3>
<p>The implementation followed four major phases:</p>
<p><strong>Phase 1: Foundation</strong>
- Established <code>Type_Safe</code> base classes
- Created initial schema structures
- Built basic timing infrastructure</p>
<p><strong>Phase 2: Schema Refinement</strong>
- Moved from <code>Safe_Str</code> to domain-specific string types
- Introduced typed collections (<code>Type_Safe__List</code>, <code>Type_Safe__Dict</code>)
- Added validation constraints</p>
<p><strong>Phase 3: Architectural Refactoring</strong>
- Separated calculation from presentation in <code>Perf_Benchmark__Diff</code>
- Introduced status enums for error handling
- Created pluggable export system</p>
<p><strong>Phase 4: Type Safety Completion</strong>
- Removed all raw <code>str</code> usage
- Added <code>@type_safe</code> decorator to all methods
- Created specialized primitives for each domain</p>
<h3 id="22-test-driven-development">2.2 Test-Driven Development<a class="headerlink" href="#22-test-driven-development" title="Permanent link">&para;</a></h3>
<p>Each component was built with comprehensive tests:</p>
<pre><code>tests/unit/helpers/performance/benchmark/
├── schemas/                           # Schema validation tests
├── export/                            # Export format tests
│   ├── test_Perf_Benchmark__Export.py
│   ├── test_Perf_Benchmark__Export__Text.py
│   ├── test_Perf_Benchmark__Export__HTML.py
│   └── test_Perf_Benchmark__Export__JSON.py
├── testing/                           # Test infrastructure tests
├── test_Perf_Benchmark__Diff.py
├── test_Perf_Benchmark__Hypothesis.py
├── test_Perf_Benchmark__Timing.py
└── test_Perf_Benchmark__Timing__Reporter.py
</code></pre>
<p><strong>Test Data Factory Pattern:</strong></p>
<pre><code class="language-python">class QA__Benchmark__Test_Data(Type_Safe):
    # Base scores for predictable testing
    base_score_1 : int = 1000
    base_score_2 : int = 500
    base_score_3 : int = 2000

    def create_session_with_scores(self, title, score_multiplier):
        &quot;&quot;&quot;Create session with scaled scores for diff testing&quot;&quot;&quot;
        # multiplier &lt; 1.0 = improvement
        # multiplier = 1.0 = baseline
        # multiplier &gt; 1.0 = regression
</code></pre>
<p>This enabled deterministic testing of comparison logic:</p>
<pre><code class="language-python">sessions = [('Session 1', 1.0),    # Baseline
            ('Session 2', 0.9),    # 10% improvement
            ('Session 3', 1.2)]    # 20% regression
</code></pre>
<h3 id="23-code-formatting-standards">2.3 Code Formatting Standards<a class="headerlink" href="#23-code-formatting-standards" title="Permanent link">&para;</a></h3>
<p>Consistent visual alignment for readability:</p>
<pre><code class="language-python">class Schema__Perf__Benchmark__Result(Type_Safe):
    benchmark_id : Safe_Str__Benchmark_Id                                        # Full ID
    section      : Safe_Str__Benchmark__Section                                  # Section code
    index        : Safe_Str__Benchmark__Index                                    # Index within section
    name         : Safe_Str__Benchmark__Title                                    # Display name
    final_score  : Safe_UInt                                                     # Rounded score (ns)
    raw_score    : Safe_UInt                                                     # Unrounded score (ns)
</code></pre>
<p>Right-aligned imports at column 102:</p>
<pre><code class="language-python">from osbot_utils.type_safe.Type_Safe                                                                      import Type_Safe
from osbot_utils.type_safe.primitives.core.Safe_UInt                                                      import Safe_UInt
</code></pre>
<h3 id="24-error-handling-pattern">2.4 Error Handling Pattern<a class="headerlink" href="#24-error-handling-pattern" title="Permanent link">&para;</a></h3>
<p><strong>Before (string-based):</strong></p>
<pre><code class="language-python">def compare_two(self) -&gt; Safe_Str:
    if len(self.sessions) &lt; 2:
        return Safe_Str('Need at least 2 sessions to compare')
    # ... calculation mixed with formatting
    return Safe_Str('\n'.join(table.text__all))
</code></pre>
<p><strong>After (schema-based):</strong></p>
<pre><code class="language-python">def compare_two(self) -&gt; Schema__Perf__Comparison__Two:
    if len(self.sessions) &lt; 2:
        return Schema__Perf__Comparison__Two(
            status = Enum__Comparison__Status.ERROR_INSUFFICIENT_SESSIONS,
            error  = 'Need at least 2 sessions to compare'
        )
    # ... pure calculation
    return Schema__Perf__Comparison__Two(
        status      = Enum__Comparison__Status.SUCCESS,
        comparisons = comparisons
    )
</code></pre>
<hr />
<h2 id="3-why-we-did-it">3. Why We Did It<a class="headerlink" href="#3-why-we-did-it" title="Permanent link">&para;</a></h2>
<h3 id="31-type-safety-benefits">3.1 Type Safety Benefits<a class="headerlink" href="#31-type-safety-benefits" title="Permanent link">&para;</a></h3>
<p><strong>Compile-time error detection:</strong></p>
<pre><code class="language-python"># This fails at assignment, not at runtime
result.score_a = &quot;not a number&quot;  # TypeError: expected Safe_UInt
</code></pre>
<p><strong>Self-documenting code:</strong></p>
<pre><code class="language-python">def format_time(self, ns_value: Safe_UInt) -&gt; Safe_Str__Time_Formatted:
    # Types declare: input is nanoseconds, output is formatted display string
</code></pre>
<p><strong>Automatic validation:</strong></p>
<pre><code class="language-python">class Safe_Str__Benchmark_Id(Safe_Str):
    max_length = 100
    regex      = re.compile(r'[^a-zA-Z0-9_]')  # Only alphanumeric + underscore
</code></pre>
<h3 id="32-separation-of-concerns">3.2 Separation of Concerns<a class="headerlink" href="#32-separation-of-concerns" title="Permanent link">&para;</a></h3>
<p><strong>Calculation layer</strong> (Perf_Benchmark__Diff):
- Pure logic returning schemas
- No formatting or presentation
- Testable independently</p>
<p><strong>Export layer</strong> (Perf_Benchmark__Export__*):
- Schema → formatted output
- Multiple formats from same data
- Pluggable architecture</p>
<p><strong>Benefits:</strong>
- Schemas can be serialized/saved independently
- Programmatic access to comparison data
- UI can render schemas directly
- API can return schemas as JSON</p>
<h3 id="33-semantic-string-types">3.3 Semantic String Types<a class="headerlink" href="#33-semantic-string-types" title="Permanent link">&para;</a></h3>
<p><strong>Why not just <code>Safe_Str</code> everywhere?</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Max Length</th>
<th>Character Set</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Safe_Str__Benchmark_Id</code></td>
<td>100</td>
<td><code>[a-zA-Z0-9_]</code></td>
<td>Machine-readable identifiers</td>
</tr>
<tr>
<td><code>Safe_Str__Benchmark__Title</code></td>
<td>200</td>
<td><code>[a-zA-Z0-9_ ()-:,.v]</code></td>
<td>Short UI headers</td>
</tr>
<tr>
<td><code>Safe_Str__Benchmark__Description</code></td>
<td>4096</td>
<td>Extended ASCII + backticks</td>
<td>Technical documentation</td>
</tr>
<tr>
<td><code>Safe_Str__Benchmark__Section</code></td>
<td>50</td>
<td><code>[a-zA-Z0-9_]</code></td>
<td>Dict keys (identifier-safe)</td>
</tr>
</tbody>
</table>
<p><strong>Different validation needs:</strong>
- IDs should be strict identifiers (no spaces, punctuation)
- Titles need limited punctuation for readability
- Descriptions need code formatting characters</p>
<h3 id="34-percentage-change-design">3.4 Percentage Change Design<a class="headerlink" href="#34-percentage-change-design" title="Permanent link">&para;</a></h3>
<p><strong>Challenge:</strong> Percentage changes are asymmetric
- Improvement: max +100% (can't be faster than 0ns)
- Regression: unbounded (code can be infinitely slower)</p>
<p><strong>Solution:</strong> Symmetric high bounds for practical use</p>
<pre><code class="language-python">class Safe_Float__Percentage_Change(Safe_Float):
    min_value      = -1_000_000.0   # Up to 10,000× slower
    max_value      =  1_000_000.0   # Up to 10,000× faster
    decimal_places = 2
    round_output   = True           # Eliminates -11.11111111111111
</code></pre>
<hr />
<h2 id="4-lessons-learned">4. Lessons Learned<a class="headerlink" href="#4-lessons-learned" title="Permanent link">&para;</a></h2>
<h3 id="41-start-with-primitives">4.1 Start with Primitives<a class="headerlink" href="#41-start-with-primitives" title="Permanent link">&para;</a></h3>
<p><strong>Anti-pattern:</strong> Using generic types then specializing later</p>
<pre><code class="language-python"># Started with
title: Safe_Str
# Had to refactor to
title: Safe_Str__Benchmark__Title
</code></pre>
<p><strong>Better approach:</strong> Define domain primitives first
- Forces thinking about constraints early
- Reduces refactoring later
- Makes intent clear from the start</p>
<h3 id="42-test-data-matters">4.2 Test Data Matters<a class="headerlink" href="#42-test-data-matters" title="Permanent link">&para;</a></h3>
<p><strong>Problem:</strong> Initial test sessions had identical data</p>
<pre><code class="language-python"># All sessions created the same way - no actual diff to test
for title in ['Session 1', 'Session 2', 'Session 3']:
    session = cls.test_data.create_session(title=title)
</code></pre>
<p><strong>Solution:</strong> Parameterized test data factory</p>
<pre><code class="language-python">def create_session_with_scores(self, title, score_multiplier):
    # Controllable scores for predictable diff results
</code></pre>
<p><strong>Lesson:</strong> Test data should exercise all code paths, not just prove code runs.</p>
<h3 id="43-separate-calculation-from-presentation">4.3 Separate Calculation from Presentation<a class="headerlink" href="#43-separate-calculation-from-presentation" title="Permanent link">&para;</a></h3>
<p><strong>Problem:</strong> Methods returning formatted strings
- Can't programmatically inspect results
- Can't save/load comparison data
- Error handling mixed with output generation</p>
<p><strong>Solution:</strong> Return schemas, export separately</p>
<pre><code class="language-python"># Calculation
result = diff.compare_two()

# Check status
if result.status == Enum__Comparison__Status.SUCCESS:
    # Multiple export options
    print(text_export.export_comparison(result))
    file_create('report.html', html_export.export_comparison(result))
    file_create('data.json', result.json())
</code></pre>
<h3 id="44-enums-over-magic-strings">4.4 Enums Over Magic Strings<a class="headerlink" href="#44-enums-over-magic-strings" title="Permanent link">&para;</a></h3>
<p><strong>Problem:</strong> String-based status checking</p>
<pre><code class="language-python">if 'Need at least 2' in str(comparison):
    # Error case
</code></pre>
<p><strong>Solution:</strong> Enum status with separate error message</p>
<pre><code class="language-python">if result.status == Enum__Comparison__Status.ERROR_INSUFFICIENT_SESSIONS:
    print(result.error)  # Human-readable message
</code></pre>
<p><strong>Benefits:</strong>
- IDE autocomplete for status values
- Typos caught at compile time
- Clear enumeration of all possible states</p>
<h3 id="45-the-type_safe-decorator">4.5 The @type_safe Decorator<a class="headerlink" href="#45-the-type_safe-decorator" title="Permanent link">&para;</a></h3>
<p><strong>Discovery:</strong> Return type conversion is automatic</p>
<pre><code class="language-python">@type_safe
def format_time(self, ns_value: Safe_UInt) -&gt; Safe_Str__Time_Formatted:
    return f'{value:,} ns'  # str auto-converted to Safe_Str__Time_Formatted
</code></pre>
<p><strong>No need for:</strong></p>
<pre><code class="language-python">return Safe_Str__Time_Formatted(f'{value:,} ns')  # Redundant
</code></pre>
<h3 id="46-dual-storage-pattern">4.6 Dual Storage Pattern<a class="headerlink" href="#46-dual-storage-pattern" title="Permanent link">&para;</a></h3>
<p><strong>Problem:</strong> Only storing summary results lost detailed measurement data</p>
<pre><code class="language-python">class Perf_Benchmark__Timing(Type_Safe):
    session : Perf  # Single reused session - only keeps last benchmark
</code></pre>
<p><strong>Solution:</strong> Store both summaries and full sessions</p>
<pre><code class="language-python">class Perf_Benchmark__Timing(Type_Safe):
    results  : Dict__Benchmark_Results    # Quick access to scores
    sessions : Dict__Benchmark_Sessions   # Full measurement data (stddev, raw_times, etc.)
</code></pre>
<p><strong>Benefits:</strong>
- Quick access: <code>timing.results['A_01'].final_score</code>
- Deep analysis: <code>timing.sessions['A_01'].result.stddev_time</code></p>
<h3 id="47-legend-vs-description-semantics">4.7 Legend vs Description Semantics<a class="headerlink" href="#47-legend-vs-description-semantics" title="Permanent link">&para;</a></h3>
<p><strong>Insight:</strong> Legend values are titles, not descriptions</p>
<pre><code class="language-python"># Legend maps section keys to display names
legend = {'A': 'Python Baselines', 'B': 'Type_Safe Measurements'}

# These are short, title-like strings
Dict__Benchmark__Legend.expected_value_type = Safe_Str__Benchmark__Title  # Not Safe_Str__Benchmark__Description
</code></pre>
<h3 id="48-print-is-just-another-export">4.8 Print is Just Another Export<a class="headerlink" href="#48-print-is-just-another-export" title="Permanent link">&para;</a></h3>
<p><strong>Realization:</strong> Console output is semantically equivalent to file export</p>
<pre><code class="language-python"># These are the same operation with different sinks
print(text_export.export_comparison(result))           # To console
file_create('report.txt', text_export.export_comparison(result))  # To file
</code></pre>
<p><strong>Architecture:</strong> Single exporter class handles both cases.</p>
<hr />
<h2 id="5-file-structure">5. File Structure<a class="headerlink" href="#5-file-structure" title="Permanent link">&para;</a></h2>
<pre><code>osbot_utils/helpers/performance/benchmark/
├── Perf_Benchmark__Diff.py
├── Perf_Benchmark__Hypothesis.py
├── Perf_Benchmark__Timing.py
├── Perf_Benchmark__Timing__Reporter.py
├── TestCase__Benchmark__Timing.py
├── export/
│   ├── Perf_Benchmark__Export.py
│   ├── Perf_Benchmark__Export__HTML.py
│   ├── Perf_Benchmark__Export__JSON.py
│   └── Perf_Benchmark__Export__Text.py
├── schemas/
│   ├── Schema__Perf__Benchmark__Comparison.py
│   ├── Schema__Perf__Benchmark__Evolution.py
│   ├── Schema__Perf__Benchmark__Result.py
│   ├── Schema__Perf__Benchmark__Session.py
│   ├── Schema__Perf__Comparison__Two.py
│   ├── Schema__Perf__Evolution.py
│   ├── Schema__Perf__Hypothesis__Result.py
│   ├── Schema__Perf__Statistics.py
│   ├── collections/
│   │   ├── Dict__Benchmark_Results.py
│   │   ├── Dict__Benchmark_Sessions.py
│   │   ├── Dict__Benchmark__Legend.py
│   │   ├── List__Benchmark_Comparisons.py
│   │   ├── List__Benchmark_Evolutions.py
│   │   ├── List__Benchmark_Sessions.py
│   │   ├── List__Scores.py
│   │   └── List__Titles.py
│   ├── enums/
│   │   ├── Enum__Benchmark__Trend.py
│   │   ├── Enum__Comparison__Status.py
│   │   ├── Enum__Hypothesis__Status.py
│   │   └── Enum__Time_Unit.py
│   ├── safe_str/
│   │   ├── Safe_Str__Benchmark_Id.py
│   │   ├── Safe_Str__Benchmark__Index.py
│   │   └── Safe_Str__Benchmark__Section.py
│   └── timing/
│       └── Schema__Perf_Benchmark__Timing__Config.py
└── testing/
    └── QA__Benchmark__Test_Data.py
</code></pre>
<hr />
<h2 id="6-usage-examples">6. Usage Examples<a class="headerlink" href="#6-usage-examples" title="Permanent link">&para;</a></h2>
<h3 id="basic-timing">Basic Timing<a class="headerlink" href="#basic-timing" title="Permanent link">&para;</a></h3>
<pre><code class="language-python">with Perf_Benchmark__Timing(config=config) as timing:
    timing.benchmark(Safe_Str__Benchmark_Id('A_01__dict_create'), dict)
    timing.benchmark(Safe_Str__Benchmark_Id('A_02__list_create'), list, assert_less_than=time_500_ns)

    # Access results
    print(timing.results['A_01__dict_create'].final_score)

    # Generate reports
    reporter = timing.reporter()
    reporter.print_summary()
    reporter.save_all()
</code></pre>
<h3 id="session-comparison">Session Comparison<a class="headerlink" href="#session-comparison" title="Permanent link">&para;</a></h3>
<pre><code class="language-python">diff = Perf_Benchmark__Diff()
diff.load_folder('/benchmarks/')

# Get structured results
comparison = diff.compare_two()
evolution  = diff.compare_all()
stats      = diff.statistics()

# Export to multiple formats
text_export = Perf_Benchmark__Export__Text()
html_export = Perf_Benchmark__Export__HTML()
json_export = Perf_Benchmark__Export__JSON()

print(text_export.export_comparison(comparison))
file_create('evolution.html', html_export.export_evolution(evolution))
file_create('stats.json', json_export.export_statistics(stats))
</code></pre>
<h3 id="hypothesis-testing">Hypothesis Testing<a class="headerlink" href="#hypothesis-testing" title="Permanent link">&para;</a></h3>
<pre><code class="language-python">hypothesis = Perf_Benchmark__Hypothesis(timing=timing, tolerance=10.0)
hypothesis.load_baseline('/baseline.json')

result = hypothesis.test_no_regression()
if result.status == Enum__Hypothesis__Status.FAILED:
    print(f&quot;Regression detected: {result.comments}&quot;)
</code></pre>
<hr />
<h2 id="7-future-considerations">7. Future Considerations<a class="headerlink" href="#7-future-considerations" title="Permanent link">&para;</a></h2>
<h3 id="potential-enhancements">Potential Enhancements<a class="headerlink" href="#potential-enhancements" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Percentile tracking</strong> - P50, P95, P99 in addition to avg/median</li>
<li><strong>Outlier detection</strong> - Automatic identification of anomalous measurements</li>
<li><strong>Trend prediction</strong> - Statistical projection of performance trajectory</li>
<li><strong>CI/CD integration</strong> - GitHub Actions / Jenkins reporter plugins</li>
<li><strong>Interactive HTML</strong> - Drill-down charts with benchmark details</li>
</ul>
<h3 id="schema-evolution">Schema Evolution<a class="headerlink" href="#schema-evolution" title="Permanent link">&para;</a></h3>
<ul>
<li>Versioning strategy for backward-compatible changes</li>
<li>Migration utilities for existing saved sessions</li>
</ul>
<hr />
<h2 id="8-conclusion">8. Conclusion<a class="headerlink" href="#8-conclusion" title="Permanent link">&para;</a></h2>
<p>The Perf_Benchmark v4 system demonstrates that comprehensive type safety and clean architecture are achievable without sacrificing usability. The key principles that made this successful:</p>
<ol>
<li><strong>Domain-specific primitives</strong> over generic types</li>
<li><strong>Schemas for data, exporters for presentation</strong></li>
<li><strong>Enums for status, strings for messages</strong></li>
<li><strong>Test data factories</strong> for predictable, comprehensive testing</li>
<li><strong>Consistent formatting</strong> for maintainable code</li>
</ol>
<p>The 300+ passing tests in 140ms prove that type safety doesn't impose significant runtime overhead, while providing substantial development-time benefits in error prevention, documentation, and IDE support.</p>
              
            </div>
          </div><footer>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
    
  </span>
</div>
    <script src="../../../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../../../..";</script>
    <script src="../../../../js/theme_extra.js"></script>
    <script src="../../../../js/theme.js"></script>
      <script src="../../../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
