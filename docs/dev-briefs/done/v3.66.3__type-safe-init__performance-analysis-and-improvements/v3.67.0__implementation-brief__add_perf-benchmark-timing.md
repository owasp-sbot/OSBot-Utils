# Implementation Brief: Perf_Benchmark__Timing Infrastructure

- **version**: v3.67.0
- **updated**: January 2026
- **status**: Ready for Implementation

---

## 1. Introduction

### 1.1 What We're Building

This document describes the implementation of a **reusable benchmarking infrastructure** for the `osbot_utils` library. The goal is to create a clean, Type_Safe framework that makes it easy to:

1. **Write performance benchmarks** with minimal boilerplate
2. **Collect and organize results** with automatic section/index extraction
3. **Generate reports** in multiple formats (text, JSON, markdown, HTML)
4. **Compare results** between runs (before/after optimization)
5. **Test hypotheses** about performance improvements
6. **Track evolution** across multiple benchmark sessions over time

```
┌────────────────────────────────────────────────────────────────────────────────────┐
│                                                                                    │
│   BEFORE: 50+ lines of boilerplate per benchmark file                              │
│                                                                                    │
│   ┌─────────────────────────────────────────────────────────────────────────────┐  │
│   │  class test_perf__Something(TestCase):                                      │  │
│   │      @classmethod                                                           │  │
│   │      def setUpClass(cls):                                                   │  │
│   │          cls.session = Perf(assert_enabled=True)                            │  │
│   │          cls.results = {}                                                   │  │
│   │          cls.time_1_kns = 1000                                              │  │
│   │          # ... more thresholds ...                                          │  │
│   │                                                                             │  │
│   │      @classmethod                                                           │  │
│   │      def tearDownClass(cls):                                                │  │
│   │          # ... 30 lines of formatting code ...                              │  │
│   │          # ... JSON serialization ...                                       │  │
│   │          # ... file writing ...                                             │  │
│   │                                                                             │  │
│   │      def capture_result(self, name, result):                                │  │
│   │          # ... result capture logic ...                                     │  │
│   │                                                                             │  │
│   │      def test_perf__A_01__something(self):                                  │  │
│   │          with self.session as _:                                            │  │
│   │              _.measure__quick(lambda: operation())                          │  │
│   │              self.capture_result('A_01__something', _.result)               │  │
│   │              _.assert_time__less_than(self.time_1_kns)                      │  │
│   └─────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                    │
│   AFTER: Clean, minimal test files                                                 │
│                                                                                    │
│   ┌─────────────────────────────────────────────────────────────────────────────┐  │
│   │  class test_perf__Something(TestCase__Benchmark__Timing):                   │  │
│   │      config = Perf_Benchmark__Timing__Config(title="My Benchmarks")         │  │
│   │                                                                             │  │
│   │      def test__A_01__something(self):                                       │  │
│   │          self.benchmark('A_01__something', lambda: operation())             │  │
│   └─────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                    │
└────────────────────────────────────────────────────────────────────────────────────┘
```

### 1.2 Why We Need This

The `osbot_utils` library is undergoing performance optimization work, particularly around `Type_Safe` object creation. To measure and track these optimizations, we need:

| Need | Solution |
|------|----------|
| **Consistent measurement** | Leverage existing `Perf` class with Fibonacci sampling |
| **Organized results** | Section/index extraction from benchmark IDs |
| **Multiple output formats** | Reporter generates txt, JSON, markdown, HTML |
| **Before/after comparison** | Built-in comparison functionality |
| **Hypothesis testing** | Structured experiments with baseline vs optimized code |
| **Evolution tracking** | Load and compare multiple sessions from disk |
| **Minimal boilerplate** | Base class handles all infrastructure |
| **Type safety** | All components use Type_Safe patterns |

### 1.3 The Performance Measurement Foundation: `Perf` Class

We're building on top of the existing `Performance_Measure__Session` class (aliased as `Perf`), which provides sophisticated timing capabilities:

```
┌────────────────────────────────────────────────────────────────────────────────────┐
│                     Performance_Measure__Session (Perf)                            │
├────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                    │
│   FIBONACCI SAMPLING                                                               │
│   ══════════════════                                                               │
│   Instead of fixed iteration counts, uses Fibonacci sequence for sampling:         │
│                                                                                    │
│   ┌─────────────────────────────────────────────────────────────────────────────┐  │
│   │  FULL:  [1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610]             │  │
│   │         = 1,597 total invocations                                           │  │
│   │                                                                             │  │
│   │  FAST:  [1, 2, 3, 5, 8, 13, 21, 34]                                         │  │
│   │         = 87 total invocations                                              │  │
│   │                                                                             │  │
│   │  QUICK: [1, 2, 3, 5, 8]                                                     │  │
│   │         = 19 total invocations  ◄── We use this for benchmarks              │  │
│   └─────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                    │
│   WHY FIBONACCI?                                                                   │
│   • Captures warm-up effects (JIT, caches)                                         │
│   • Natural progression of sample sizes                                            │
│   • Statistical stability across different scales                                  │
│                                                                                    │
├────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                    │
│   SCORE CALCULATION                                                                │
│   ═════════════════                                                                │
│                                                                                    │
│   ┌─────────────────────────────────────────────────────────────────────────────┐  │
│   │                                                                             │  │
│   │   Raw Times ──► Sort ──► Trim 10% ──► Weighted Score ──► Normalize          │  │
│   │       │                  each end      (60% median +      to bucket         │  │
│   │       │                                 40% mean)                           │  │
│   │       ▼                                                                     │  │
│   │   [523, 487, 512, 498, 1205, 489, 501, 495, 510, 488]                       │  │
│   │       │                                                                     │  │
│   │       ▼ sort                                                                │  │
│   │   [487, 488, 489, 495, 498, 501, 510, 512, 523, 1205]                       │  │
│   │       │                                                                     │  │
│   │       ▼ trim outliers (10% each end)                                        │  │
│   │   [489, 495, 498, 501, 510, 512, 523]                                       │  │
│   │       │                                                                     │  │
│   │       ▼ weighted: 60% median (501) + 40% mean (504)                         │  │
│   │   raw_score = 502                                                           │  │
│   │       │                                                                     │  │
│   │       ▼ normalize to bucket                                                 │  │
│   │   final_score = 500  (nearest 100ns for sub-µs values)                      │  │
│   │                                                                             │  │
│   └─────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                    │
├────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                    │
│   NORMALIZATION BUCKETS                                                            │
│   ═════════════════════                                                            │
│                                                                                    │
│   Score Range          Bucket Size       Example                                   │
│   ───────────────────────────────────────────────────                              │
│   < 1,000 ns           100 ns            487 → 500                                 │
│   < 10,000 ns          1,000 ns          5,234 → 5,000                             │
│   < 100,000 ns         10,000 ns         52,340 → 50,000                           │
│   ≥ 100,000 ns         100,000 ns        523,400 → 500,000                         │
│                                                                                    │
│   WHY? Makes scores stable across runs and comparable between machines             │
│                                                                                    │
├────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                    │
│   KEY METHODS                                                                      │
│   ═══════════                                                                      │
│                                                                                    │
│   measure(target)        Full measurement (1,597 invocations)                      │
│   measure__fast(target)  Balanced measurement (87 invocations)                     │
│   measure__quick(target) Quick measurement (19 invocations) ◄── Our default        │
│                                                                                    │
│   assert_time(*expected)        Score must match one of expected values            │
│   assert_time__less_than(max)   Score must be below threshold                      │
│   assert_time__more_than(min)   Score must be above threshold                      │
│                                                                                    │
│   print()                       Print single-line result                           │
│   print_report()                Print detailed report with histogram               │
│                                                                                    │
└────────────────────────────────────────────────────────────────────────────────────┘
```

### 1.4 What We're Adding

We're wrapping the `Perf` class with infrastructure that handles:

```
┌────────────────────────────────────────────────────────────────────────────────────┐
│                                                                                    │
│                        NEW BENCHMARKING INFRASTRUCTURE                             │
│                                                                                    │
│   ┌─────────────────────────────────────────────────────────────────────────────┐  │
│   │                                                                             │  │
│   │   USER CODE                        INFRASTRUCTURE                           │  │
│   │   ─────────                        ──────────────                           │  │
│   │                                                                             │  │
│   │   benchmark('A_01__nop', fn)  ───► Perf_Benchmark__Timing                   │  │
│   │                                         │                                   │  │
│   │                                         ├── Calls Perf.measure__quick()     │  │
│   │                                         │                                   │  │
│   │                                         ├── Parses ID → section/index       │  │
│   │                                         │   'A_01__nop' → A, 01, nop        │  │
│   │                                         │                                   │  │
│   │                                         ├── Creates Schema result           │  │
│   │                                         │                                   │  │
│   │                                         └── Stores in Dict__Results         │  │
│   │                                                   │                         │  │
│   │                                                   ▼                         │  │
│   │                                         Perf_Benchmark__Timing__Reporter    │  │
│   │                                                   │                         │  │
│   │                                         ┌────┬────┼────┬────┐               │  │
│   │                                         ▼    ▼    ▼    ▼    ▼               │  │
│   │                                      .txt .json .md .html  diff             │  │
│   │                                                                             │  │
│   └─────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                    │
└────────────────────────────────────────────────────────────────────────────────────┘
```

### 1.5 Core Concepts

This infrastructure introduces three key concepts:

```
┌────────────────────────────────────────────────────────────────────────────────────┐
│                                                                                    │
│                              THREE CORE CONCEPTS                                   │
│                                                                                    │
│   ┌─────────────────────────────────────────────────────────────────────────────┐  │
│   │                                                                             │  │
│   │   1. BENCHMARK SESSION                                                      │  │
│   │   ════════════════════                                                      │  │
│   │                                                                             │  │
│   │   A single execution of a benchmark suite. Captures multiple individual     │  │
│   │   benchmark measurements in one run.                                        │  │
│   │                                                                             │  │
│   │   ┌──────────────────────────────────────────────────────────────────────┐  │  │
│   │   │  Session: "Type_Safe Performance - 2026-01-06"                       │  │  │
│   │   │  ├── A_01__python__nop           : 100 ns                            │  │  │
│   │   │  ├── A_02__python__class_empty   : 200 ns                            │  │  │
│   │   │  ├── B_01__type_safe__empty      : 800 ns                            │  │  │
│   │   │  └── B_02__type_safe__primitives : 5,000 ns                          │  │  │
│   │   └──────────────────────────────────────────────────────────────────────┘  │  │
│   │                                                                             │  │
│   │   Output: JSON file that can be loaded later for comparison                 │  │
│   │                                                                             │  │
│   └─────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                    │
│   ┌─────────────────────────────────────────────────────────────────────────────┐  │
│   │                                                                             │  │
│   │   2. HYPOTHESIS                                                             │  │
│   │   ═════════════                                                             │  │
│   │                                                                             │  │
│   │   A structured experiment to test a performance improvement claim.          │  │
│   │   By definition, a hypothesis has exactly TWO executions:                   │  │
│   │                                                                             │  │
│   │   ┌──────────────────────────────────────────────────────────────────────┐  │  │
│   │   │                                                                      │  │  │
│   │   │   Hypothesis: "skip_setattr reduces Type_Safe creation by 50%"       │  │  │
│   │   │                                                                      │  │  │
│   │   │   ┌─────────────────┐         ┌─────────────────┐                    │  │  │
│   │   │   │    BEFORE       │         │     AFTER       │                    │  │  │
│   │   │   │   (baseline)    │   vs    │  (optimized)    │                    │  │  │
│   │   │   │                 │         │                 │                    │  │  │
│   │   │   │  Code unchanged │         │  Code modified  │                    │  │  │
│   │   │   │  5,000 ns       │         │  2,000 ns       │                    │  │  │
│   │   │   └─────────────────┘         └─────────────────┘                    │  │  │
│   │   │                                                                      │  │  │
│   │   │   Result: SUCCESS (-60% improvement, exceeds 50% target)             │  │  │
│   │   │                                                                      │  │  │
│   │   └──────────────────────────────────────────────────────────────────────┘  │  │
│   │                                                                             │  │
│   │   KEY PRINCIPLE: Always measure BEFORE touching the codebase!               │  │
│   │   The baseline must be captured with unchanged code.                        │  │
│   │                                                                             │  │
│   └─────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                    │
│   ┌─────────────────────────────────────────────────────────────────────────────┐  │
│   │                                                                             │  │
│   │   3. DIFF (Multi-Session Comparison)                                        │  │
│   │   ══════════════════════════════════                                        │  │
│   │                                                                             │  │
│   │   Compare 2 or more sessions to track performance evolution over time.      │  │
│   │   Sessions are loaded from saved JSON files.                                │  │
│   │                                                                             │  │
│   │   ┌──────────────────────────────────────────────────────────────────────┐  │  │
│   │   │                                                                      │  │  │
│   │   │   Load from folder: /benchmarks/type_safe/                           │  │  │
│   │   │   ├── session_2026_01_01.json                                        │  │  │
│   │   │   ├── session_2026_01_03.json                                        │  │  │
│   │   │   ├── session_2026_01_05.json                                        │  │  │
│   │   │   └── session_2026_01_06.json                                        │  │  │
│   │   │                                                                      │  │  │
│   │   │   ┌───────────────────────────────────────────────────────────┐      │  │  │
│   │   │   │  Benchmark         │ Jan 1  │ Jan 3  │ Jan 5  │ Jan 6     │      │  │  │
│   │   │   │───────────────────────────────────────────────────────────│      │  │  │
│   │   │   │  type_safe__empty  │ 800 ns │ 600 ns │ 400 ns │ 300 ns ▼  │      │  │  │
│   │   │   │  type_safe__prims  │ 5 µs   │ 4 µs   │ 2 µs   │ 500 ns ▼  │      │  │  │
│   │   │   └───────────────────────────────────────────────────────────┘      │  │  │
│   │   │                                                                      │  │  │
│   │   └──────────────────────────────────────────────────────────────────────┘  │  │
│   │                                                                             │  │
│   │   KEY INSIGHT: The JSON files we save may be the ONLY thing that survives   │  │
│   │   across versions. The Diff tool can reconstruct analysis from saved files. │  │
│   │                                                                             │  │
│   └─────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                    │
└────────────────────────────────────────────────────────────────────────────────────┘
```

---

## 2. Architecture Overview

### 2.1 Component Relationships

```
┌────────────────────────────────────────────────────────────────────────────────────┐
│                                                                                    │
│                              COMPONENT ARCHITECTURE                                │
│                                                                                    │
│   ┌─────────────────────────────────────────────────────────────────────────────┐  │
│   │                                                                             │  │
│   │                    TestCase__Benchmark__Timing                              │  │
│   │                    (extends unittest.TestCase)                              │  │
│   │                                                                             │  │
│   │    ┌─────────────────────────────────────────────────────────────────────┐  │  │
│   │    │  • setUpClass()   → creates timing, calls start()                   │  │  │
│   │    │  • tearDownClass() → calls stop(), prints summary                   │  │  │
│   │    │  • benchmark()    → delegates to self.timing.benchmark()            │  │  │
│   │    │  • reporter()     → delegates to self.timing.reporter()             │  │  │
│   │    └─────────────────────────────────────────────────────────────────────┘  │  │
│   │                                      │                                      │  │
│   │                                      │ has-a                                │  │
│   │                                      ▼                                      │  │
│   │    ┌─────────────────────────────────────────────────────────────────────┐  │  │
│   │    │                                                                     │  │  │
│   │    │                    Perf_Benchmark__Timing                           │  │  │
│   │    │                    (extends Type_Safe)                              │  │  │
│   │    │                                                                     │  │  │
│   │    │  ┌────────────────────────────────────────────────────────────────┐ │  │  │
│   │    │  │  config  : Perf_Benchmark__Timing__Config                      │ │  │  │
│   │    │  │  results : Dict__Benchmark_Results                             │ │  │  │
│   │    │  │  session : Performance_Measure__Session (Perf)                 │ │  │  │
│   │    │  ├────────────────────────────────────────────────────────────────┤ │  │  │
│   │    │  │  start()     → initialize session                              │ │  │  │
│   │    │  │  stop()      → finalize, auto-save if configured               │ │  │  │
│   │    │  │  benchmark() → measure + capture result                        │ │  │  │
│   │    │  │  reporter()  → create reporter instance                        │ │  │  │
│   │    │  │  __enter__() → context manager (calls start)                   │ │  │  │
│   │    │  │  __exit__()  → context manager (calls stop)                    │ │  │  │
│   │    │  └────────────────────────────────────────────────────────────────┘ │  │  │
│   │    │                         │                                           │  │  │
│   │    └─────────────────────────┼───────────────────────────────────────────┘  │  │
│   │                              │                                              │  │
│   │                              │ creates                                      │  │
│   │                              ▼                                              │  │
│   │    ┌─────────────────────────────────────────────────────────────────────┐  │  │
│   │    │                                                                     │  │  │
│   │    │               Perf_Benchmark__Timing__Reporter                      │  │  │
│   │    │               (extends Type_Safe)                                   │  │  │
│   │    │                                                                     │  │  │
│   │    │  ┌────────────────────────────────────────────────────────────────┐ │  │  │
│   │    │  │  results : Dict__Benchmark_Results                             │ │  │  │
│   │    │  │  config  : Perf_Benchmark__Timing__Config                      │ │  │  │
│   │    │  ├────────────────────────────────────────────────────────────────┤ │  │  │
│   │    │  │  build_text()     → formatted text (uses Print_Table)          │ │  │  │
│   │    │  │  build_json()     → dict for serialization                     │ │  │  │
│   │    │  │  build_markdown() → markdown tables                            │ │  │  │
│   │    │  │  build_html()     → HTML with JavaScript charts                │ │  │  │
│   │    │  │  save_all()       → write all formats to files                 │ │  │  │
│   │    │  │  compare()        → before/after comparison (2 sessions)       │ │  │  │
│   │    │  │  print_summary()  → print to console                           │ │  │  │
│   │    │  └────────────────────────────────────────────────────────────────┘ │  │  │
│   │    │                                                                     │  │  │
│   │    └─────────────────────────────────────────────────────────────────────┘  │  │
│   │                                                                             │  │
│   └─────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                    │
│   ┌─────────────────────────────────────────────────────────────────────────────┐  │
│   │                                                                             │  │
│   │                    Perf_Benchmark__Hypothesis                               │  │
│   │                    (extends Type_Safe)                                      │  │
│   │                                                                             │  │
│   │    ┌─────────────────────────────────────────────────────────────────────┐  │  │
│   │    │  description    : Safe_Str                                          │  │  │
│   │    │  before_results : Dict__Benchmark_Results                           │  │  │
│   │    │  after_results  : Dict__Benchmark_Results                           │  │  │
│   │    │  target_improvement : Safe_Float  (e.g., 0.5 for 50%)               │  │  │
│   │    ├─────────────────────────────────────────────────────────────────────┤  │  │
│   │    │  run_before(benchmarks: Callable)  → capture baseline               │  │  │
│   │    │  run_after(benchmarks: Callable)   → capture optimized              │  │  │
│   │    │  evaluate() → Schema__Perf__Hypothesis__Result                      │  │  │
│   │    │  save()     → persist to disk                                       │  │  │
│   │    └─────────────────────────────────────────────────────────────────────┘  │  │
│   │                                                                             │  │
│   └─────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                    │
│   ┌─────────────────────────────────────────────────────────────────────────────┐  │
│   │                                                                             │  │
│   │                    Perf_Benchmark__Diff                                     │  │
│   │                    (extends Type_Safe)                                      │  │
│   │                                                                             │  │
│   │    ┌─────────────────────────────────────────────────────────────────────┐  │  │
│   │    │  sessions : List__Benchmark_Sessions                                │  │  │
│   │    ├─────────────────────────────────────────────────────────────────────┤  │  │
│   │    │  load_session(filepath)  → load single JSON                         │  │  │
│   │    │  load_folder(path)       → load all JSONs from directory            │  │  │
│   │    │  compare_two()           → 2-session comparison                     │  │  │
│   │    │  compare_all()           → multi-session evolution                  │  │  │
│   │    │  build_evolution_html()  → HTML with charts                         │  │  │
│   │    │  build_statistics()      → summary stats                            │  │  │
│   │    └─────────────────────────────────────────────────────────────────────┘  │  │
│   │                                                                             │  │
│   └─────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                    │
└────────────────────────────────────────────────────────────────────────────────────┘
```

### 2.2 Data Flow

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                                                                                     │
│                                    DATA FLOW                                        │
│                                                                                     │
│                                                                                     │
│    TEST METHOD                                                                      │
│    ───────────                                                                      │
│    def test__A_01__nop(self):                                                       │
│        self.benchmark('A_01__nop', lambda: None)                                    │
│                  │                                                                  │
│                  ▼                                                                  │
│    ┌─────────────────────────────────────────────────────────────────────────────┐  │
│    │  Perf_Benchmark__Timing.benchmark()                                         │  │
│    └─────────────────────────────────────────────────────────────────────────────┘  │
│                  │                                                                  │
│                  ▼                                                                  │
│    ┌─────────────────────────────────────────────────────────────────────────────┐  │
│    │  1. Call Perf.measure__quick(target)                                        │  │
│    │                                                                             │  │
│    │     ┌────────────────────────────────────────────────────────────────────┐  │  │
│    │     │  Loop: [1, 2, 3, 5, 8] iterations                                  │  │  │
│    │     │  For each: time.perf_counter_ns() around target()                  │  │  │
│    │     │  Collect all times → calculate raw_score → normalize               │  │  │
│    │     └────────────────────────────────────────────────────────────────────┘  │  │
│    │                                                                             │  │
│    │     Returns: session.result (Model__Performance_Measure__Result)            │  │
│    │              • final_score: 100                                             │  │
│    │              • raw_score: 87                                                │  │
│    └─────────────────────────────────────────────────────────────────────────────┘  │
│                  │                                                                  │
│                  ▼                                                                  │
│    ┌─────────────────────────────────────────────────────────────────────────────┐  │
│    │  2. Parse benchmark ID                                                      │  │
│    │                                                                             │  │
│    │     'A_01__python__nop'                                                     │  │
│    │           │                                                                 │  │
│    │           ▼                                                                 │  │
│    │     ┌─────────┬─────────┬─────────────────┐                                 │  │
│    │     │ section │  index  │      name       │                                 │  │
│    │     │   'A'   │  '01'   │  'python__nop'  │                                 │  │
│    │     └─────────┴─────────┴─────────────────┘                                 │  │
│    └─────────────────────────────────────────────────────────────────────────────┘  │
│                  │                                                                  │
│                  ▼                                                                  │
│    ┌─────────────────────────────────────────────────────────────────────────────┐  │
│    │  3. Create Schema__Perf__Benchmark__Result                                  │  │
│    │                                                                             │  │
│    │     Schema__Perf__Benchmark__Result(                                        │  │
│    │         benchmark_id = 'A_01__python__nop'                                  │  │
│    │         section      = 'A'                                                  │  │
│    │         index        = '01'                                                 │  │
│    │         name         = 'python__nop'                                        │  │
│    │         final_score  = 100                                                  │  │
│    │         raw_score    = 87                                                   │  │
│    │     )                                                                       │  │
│    └─────────────────────────────────────────────────────────────────────────────┘  │
│                  │                                                                  │
│                  ▼                                                                  │
│    ┌─────────────────────────────────────────────────────────────────────────────┐  │
│    │  4. Store in Dict__Benchmark_Results                                        │  │
│    │                                                                             │  │
│    │     self.results['A_01__python__nop'] = result                              │  │
│    └─────────────────────────────────────────────────────────────────────────────┘  │
│                  │                                                                  │
│                  ▼                                                                  │
│    ┌─────────────────────────────────────────────────────────────────────────────┐  │
│    │  5. Optional: Assert time threshold                                         │  │
│    │                                                                             │  │
│    │     if assert_less_than:                                                    │  │
│    │         session.assert_time__less_than(threshold)                           │  │
│    └─────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                     │
│                                                                                     │
│    AFTER ALL TESTS (tearDownClass)                                                  │
│    ───────────────────────────────                                                  │
│                  │                                                                  │
│                  ▼                                                                  │
│    ┌─────────────────────────────────────────────────────────────────────────────┐  │
│    │  Reporter generates outputs from Dict__Benchmark_Results                    │  │
│    │                                                                             │  │
│    │     ┌────────────┐ ┌────────────┐ ┌────────────┐ ┌────────────┐             │  │
│    │     │   .txt     │ │   .json    │ │   .md      │ │   .html    │             │  │
│    │     │            │ │            │ │            │ │            │             │  │
│    │     │ Print_Table│ │ { results, │ │ # Title    │ │ <html>     │             │  │
│    │     │ formatted  │ │   sections,│ │ ## Sec A   │ │ Chart.js   │             │  │
│    │     │ output     │ │   ... }    │ │ | ID |... |│ │ graphs     │             │  │
│    │     └────────────┘ └────────────┘ └────────────┘ └────────────┘             │  │
│    └─────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### 2.3 Lifecycle

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                                                                                     │
│                              BENCHMARK LIFECYCLE                                    │
│                                                                                     │
│                                                                                     │
│   OPTION 1: TestCase (for pytest/unittest)                                          │
│   ════════════════════════════════════════                                          │
│                                                                                     │
│   ┌─────────┐    ┌─────────┐    ┌─────────┐    ┌─────────┐    ┌─────────┐           │
│   │setUpCls │───►│ test 1  │───►│ test 2  │───►│ test N  │───►│tearDown │           │
│   │         │    │         │    │         │    │         │    │  Cls    │           │
│   └────┬────┘    └────┬────┘    └────┬────┘    └────┬────┘    └────┬────┘           │
│        │              │              │              │              │                │
│        ▼              ▼              ▼              ▼              ▼                │
│   ┌─────────┐    ┌─────────┐    ┌─────────┐    ┌─────────┐    ┌─────────┐           │
│   │ start() │    │benchmark│    │benchmark│    │benchmark│    │ stop()  │           │
│   │         │    │   ()    │    │   ()    │    │   ()    │    │ print() │           │
│   │         │    │         │    │         │    │         │    │ save()? │           │
│   └─────────┘    └─────────┘    └─────────┘    └─────────┘    └─────────┘           │
│                                                                                     │
│                                                                                     │
│   OPTION 2: Context Manager (for ad-hoc use)                                        │
│   ══════════════════════════════════════════                                        │
│                                                                                     │
│   with Perf_Benchmark__Timing(config=config) as timing:                             │
│        │                                                                            │
│        │  ┌──────────────────────────────────────────────────────────────────────┐  │
│        │  │                                                                      │  │
│        │  │    __enter__()  ◄─────  calls start()                                │  │
│        │  │         │                                                            │  │
│        │  │         ▼                                                            │  │
│        │  │    timing.benchmark('A_01__test', fn)                                │  │
│        │  │    timing.benchmark('A_02__test', fn)                                │  │
│        │  │    timing.benchmark('A_03__test', fn)                                │  │
│        │  │         │                                                            │  │
│        │  │         ▼                                                            │  │
│        │  │    __exit__()  ◄─────  calls stop()                                  │  │
│        │  │                        (auto-saves if configured)                    │  │
│        │  │                                                                      │  │
│        │  └──────────────────────────────────────────────────────────────────────┘  │
│        │                                                                            │
│                                                                                     │
│                                                                                     │
│   OPTION 3: Manual Control                                                          │
│   ════════════════════════                                                          │
│                                                                                     │
│   timing = Perf_Benchmark__Timing(config=config)                                    │
│   timing.start()                                                                    │
│        │                                                                            │
│        ▼                                                                            │
│   timing.benchmark('A_01__test', fn)                                                │
│   timing.benchmark('A_02__test', fn)                                                │
│        │                                                                            │
│        ▼                                                                            │
│   timing.stop()                                                                     │
│   timing.reporter().save_all()   ◄─────  explicit save                              │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### 2.4 Hypothesis Workflow

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                                                                                     │
│                             HYPOTHESIS WORKFLOW                                     │
│                                                                                     │
│   ┌──────────────────────────────────────────────────────────────────────────────┐  │
│   │                                                                              │  │
│   │   STEP 1: Define Hypothesis                                                  │  │
│   │   ─────────────────────────                                                  │  │
│   │                                                                              │  │
│   │   hypothesis = Perf_Benchmark__Hypothesis(                                   │  │
│   │       description        = "skip_setattr reduces creation time by 50%"     , │  │
│   │       target_improvement = 0.5                                             ) │  │
│   │                                                                              │  │
│   └──────────────────────────────────────────────────────────────────────────────┘  │
│                  │                                                                  │
│                  ▼                                                                  │
│   ┌──────────────────────────────────────────────────────────────────────────────┐  │
│   │                                                                              │  │
│   │   STEP 2: Capture Baseline (BEFORE changing code!)                           │  │
│   │   ─────────────────────────────────────────────────                          │  │
│   │                                                                              │  │
│   │   def run_benchmarks(timing):                                                │  │
│   │       timing.benchmark('type_safe__empty', lambda: TS__Empty())              │  │
│   │       timing.benchmark('type_safe__prims', lambda: TS__Prims())              │  │
│   │                                                                              │  │
│   │   hypothesis.run_before(run_benchmarks)                                      │  │
│   │                                                                              │  │
│   │   ┌───────────────────────────────────────────────────────────────────────┐  │  │
│   │   │  BASELINE CAPTURED                                                    │  │  │
│   │   │  • type_safe__empty  : 800 ns                                         │  │  │
│   │   │  • type_safe__prims  : 5,000 ns                                       │  │  │
│   │   └───────────────────────────────────────────────────────────────────────┘  │  │
│   │                                                                              │  │
│   └──────────────────────────────────────────────────────────────────────────────┘  │
│                  │                                                                  │
│                  ▼                                                                  │
│   ┌──────────────────────────────────────────────────────────────────────────────┐  │
│   │                                                                              │  │
│   │   STEP 3: Make Code Changes                                                  │  │
│   │   ──────────────────────────                                                 │  │
│   │                                                                              │  │
│   │   # Modify Type_Safe.__init__ to check config.skip_setattr                   │  │
│   │   # ... implement the optimization ...                                       │  │
│   │                                                                              │  │
│   └──────────────────────────────────────────────────────────────────────────────┘  │
│                  │                                                                  │
│                  ▼                                                                  │
│   ┌──────────────────────────────────────────────────────────────────────────────┐  │
│   │                                                                              │  │
│   │   STEP 4: Capture After                                                      │  │
│   │   ─────────────────────                                                      │  │
│   │                                                                              │  │
│   │   hypothesis.run_after(run_benchmarks)                                       │  │
│   │                                                                              │  │
│   │   ┌───────────────────────────────────────────────────────────────────────┐  │  │
│   │   │  AFTER CAPTURED                                                       │  │  │
│   │   │  • type_safe__empty  : 300 ns                                         │  │  │
│   │   │  • type_safe__prims  : 500 ns                                         │  │  │
│   │   └───────────────────────────────────────────────────────────────────────┘  │  │
│   │                                                                              │  │
│   └──────────────────────────────────────────────────────────────────────────────┘  │
│                  │                                                                  │
│                  ▼                                                                  │
│   ┌──────────────────────────────────────────────────────────────────────────────┐  │
│   │                                                                              │  │
│   │   STEP 5: Evaluate                                                           │  │
│   │   ────────────────                                                           │  │
│   │                                                                              │  │
│   │   result = hypothesis.evaluate()                                             │  │
│   │                                                                              │  │
│   │   ┌───────────────────────────────────────────────────────────────────────┐  │  │
│   │   │  HYPOTHESIS RESULT                                                    │  │  │
│   │   │                                                                       │  │  │
│   │   │  Description: "skip_setattr reduces creation time by 50%"             │  │  │
│   │   │  Target: 50% improvement                                              │  │  │
│   │   │                                                                       │  │  │
│   │   │  Benchmark            Before    After     Change                      │  │  │
│   │   │  ─────────────────────────────────────────────────                    │  │  │
│   │   │  type_safe__empty     800 ns    300 ns    -62.5% ▼                    │  │  │
│   │   │  type_safe__prims     5,000 ns  500 ns    -90.0% ▼                    │  │  │
│   │   │                                                                       │  │  │
│   │   │  Average improvement: 76.25%                                          │  │  │
│   │   │                                                                       │  │  │
│   │   │  ╔═══════════════════════════════════════════════════════════════╗    │  │  │
│   │   │  ║  RESULT: ✓ SUCCESS (76.25% > 50% target)                      ║    │  │  │
│   │   │  ╚═══════════════════════════════════════════════════════════════╝    │  │  │
│   │   └───────────────────────────────────────────────────────────────────────┘  │  │
│   │                                                                              │  │
│   └──────────────────────────────────────────────────────────────────────────────┘  │
│                  │                                                                  │
│                  ▼                                                                  │
│   ┌──────────────────────────────────────────────────────────────────────────────┐  │
│   │                                                                              │  │
│   │   STEP 6: Save (optional)                                                    │  │
│   │   ───────────────────────                                                    │  │
│   │                                                                              │  │
│   │   hypothesis.save('/benchmarks/hypotheses/skip_setattr_2026_01_06.json')     │  │
│   │                                                                              │  │
│   └──────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### 2.5 Diff Workflow

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                                                                                     │
│                               DIFF WORKFLOW                                         │
│                                                                                     │
│   ┌─────────────────────────────────────────────────────────────────────────────┐  │
│   │                                                                             │  │
│   │   LOADING SESSIONS                                                          │  │
│   │   ════════════════                                                          │  │
│   │                                                                             │  │
│   │   diff = Perf_Benchmark__Diff()                                             │  │
│   │                                                                             │  │
│   │   # Option A: Load individual files                                         │  │
│   │   diff.load_session('/benchmarks/session_jan_01.json')                      │  │
│   │   diff.load_session('/benchmarks/session_jan_03.json')                      │  │
│   │   diff.load_session('/benchmarks/session_jan_06.json')                      │  │
│   │                                                                             │  │
│   │   # Option B: Load all from folder                                          │  │
│   │   diff.load_folder('/benchmarks/type_safe/')                                │  │
│   │                                                                             │  │
│   │   ┌─────────────────────────────────────────────────────────────────────┐  │  │
│   │   │  /benchmarks/type_safe/                                             │  │  │
│   │   │  ├── session_2026_01_01.json  ──► loaded                            │  │  │
│   │   │  ├── session_2026_01_03.json  ──► loaded                            │  │  │
│   │   │  ├── session_2026_01_05.json  ──► loaded                            │  │  │
│   │   │  ├── session_2026_01_06.json  ──► loaded                            │  │  │
│   │   │  └── README.md                ──► skipped (not JSON)                │  │  │
│   │   └─────────────────────────────────────────────────────────────────────┘  │  │
│   │                                                                             │  │
│   └─────────────────────────────────────────────────────────────────────────────┘  │
│                  │                                                                  │
│                  ▼                                                                  │
│   ┌─────────────────────────────────────────────────────────────────────────────┐  │
│   │                                                                             │  │
│   │   TWO-SESSION COMPARISON                                                    │  │
│   │   ══════════════════════                                                    │  │
│   │                                                                             │  │
│   │   result = diff.compare_two(session_0, session_1)                           │  │
│   │                                                                             │  │
│   │   ┌─────────────────────────────────────────────────────────────────────┐  │  │
│   │   │  Comparison: Jan 1 vs Jan 6                                         │  │  │
│   │   │                                                                     │  │  │
│   │   │  Benchmark            Jan 1       Jan 6       Change                │  │  │
│   │   │  ─────────────────────────────────────────────────────              │  │  │
│   │   │  type_safe__empty     800 ns      300 ns      -62.5% ▼              │  │  │
│   │   │  type_safe__prims     5,000 ns    500 ns      -90.0% ▼              │  │  │
│   │   │  type_safe__nested    20,000 ns   1,000 ns    -95.0% ▼              │  │  │
│   │   │                                                                     │  │  │
│   │   │  Average: -82.5% improvement                                        │  │  │
│   │   └─────────────────────────────────────────────────────────────────────┘  │  │
│   │                                                                             │  │
│   └─────────────────────────────────────────────────────────────────────────────┘  │
│                  │                                                                  │
│                  ▼                                                                  │
│   ┌─────────────────────────────────────────────────────────────────────────────┐  │
│   │                                                                             │  │
│   │   MULTI-SESSION EVOLUTION                                                   │  │
│   │   ═══════════════════════                                                   │  │
│   │                                                                             │  │
│   │   result = diff.compare_all()                                               │  │
│   │                                                                             │  │
│   │   ┌─────────────────────────────────────────────────────────────────────┐  │  │
│   │   │  Evolution: 4 sessions                                              │  │  │
│   │   │                                                                     │  │  │
│   │   │  Benchmark         Jan 1    Jan 3    Jan 5    Jan 6    Trend        │  │  │
│   │   │  ────────────────────────────────────────────────────────────       │  │  │
│   │   │  type_safe__empty  800 ns   600 ns   400 ns   300 ns   ▼▼▼         │  │  │
│   │   │  type_safe__prims  5 µs     4 µs     2 µs     500 ns   ▼▼▼         │  │  │
│   │   │  type_safe__nested 20 µs    15 µs    5 µs     1 µs     ▼▼▼         │  │  │
│   │   │                                                                     │  │  │
│   │   │  Summary:                                                           │  │  │
│   │   │  • All benchmarks improved                                          │  │  │
│   │   │  • Biggest gain: type_safe__nested (-95%)                           │  │  │
│   │   │  • Steady improvement across all sessions                           │  │  │
│   │   └─────────────────────────────────────────────────────────────────────┘  │  │
│   │                                                                             │  │
│   └─────────────────────────────────────────────────────────────────────────────┘  │
│                  │                                                                  │
│                  ▼                                                                  │
│   ┌─────────────────────────────────────────────────────────────────────────────┐  │
│   │                                                                             │  │
│   │   HTML OUTPUT WITH CHARTS                                                   │  │
│   │   ═══════════════════════                                                   │  │
│   │                                                                             │  │
│   │   diff.build_evolution_html('/reports/type_safe_evolution.html')            │  │
│   │                                                                             │  │
│   │   ┌─────────────────────────────────────────────────────────────────────┐  │  │
│   │   │  <html>                                                             │  │  │
│   │   │    <script src="chart.js">                                          │  │  │
│   │   │                                                                     │  │  │
│   │   │    ┌───────────────────────────────────────────────────────────┐   │  │  │
│   │   │    │      Performance Evolution                                │   │  │  │
│   │   │    │  ns                                                       │   │  │  │
│   │   │    │  ▲                                                        │   │  │  │
│   │   │    │  │  ●                                                     │   │  │  │
│   │   │    │  │   ╲                                                    │   │  │  │
│   │   │    │  │    ●                                                   │   │  │  │
│   │   │    │  │     ╲                                                  │   │  │  │
│   │   │    │  │      ●                                                 │   │  │  │
│   │   │    │  │       ╲                                                │   │  │  │
│   │   │    │  │        ●                                               │   │  │  │
│   │   │    │  └──────────────────────────────────────────────────► date│   │  │  │
│   │   │    │    Jan 1   Jan 3   Jan 5   Jan 6                          │   │  │  │
│   │   │    └───────────────────────────────────────────────────────────┘   │  │  │
│   │   │                                                                     │  │  │
│   │   │  </html>                                                            │  │  │
│   │   └─────────────────────────────────────────────────────────────────────┘  │  │
│   │                                                                             │  │
│   └─────────────────────────────────────────────────────────────────────────────┘  │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

---

## 3. File Structure

```
osbot_utils/testing/performance/benchmark/
│
├── __init__.py
│
├── Perf_Benchmark__Timing.py                              # Core benchmark logic
├── Perf_Benchmark__Timing__Config.py                      # Configuration schema  
├── Perf_Benchmark__Timing__Reporter.py                    # Output generation
├── TestCase__Benchmark__Timing.py                         # TestCase subclass
│
├── Perf_Benchmark__Hypothesis.py                          # Hypothesis testing
├── Perf_Benchmark__Diff.py                                # Multi-session comparison
│
└── schemas/
    │
    ├── __init__.py
    │
    ├── Schema__Perf__Benchmark__Result.py                 # Single benchmark result
    ├── Schema__Perf__Benchmark__Session.py                # Full session data
    ├── Schema__Perf__Hypothesis__Result.py                # Hypothesis outcome
    │
    ├── safe_str/
    │   ├── __init__.py
    │   ├── Safe_Str__Benchmark_Id.py                      # "A_01__python__nop"
    │   ├── Safe_Str__Benchmark__Section.py                # "A", "Python"  
    │   └── Safe_Str__Benchmark__Index.py                  # "01", "02"
    │
    ├── collections/
    │   ├── __init__.py
    │   ├── Dict__Benchmark_Results.py                     # {id → result}
    │   ├── Dict__Benchmark__Legend.py                     # {section → description}
    │   └── List__Benchmark_Sessions.py                    # [session, session, ...]
    │
    └── enums/
        ├── __init__.py
        ├── Enum__Time_Unit.py                             # NANOSECONDS, MICROSECONDS, etc.
        └── Enum__Hypothesis__Status.py                    # SUCCESS, FAILURE, INCONCLUSIVE
```

---

## 4. Schema Definitions

### 4.1 Custom Primitives

**`Safe_Str__Benchmark_Id`** - Full benchmark identifier
```python
# File: benchmark/schemas/safe_str/Safe_Str__Benchmark_Id.py
from osbot_utils.type_safe.primitives.core.Safe_Str import Safe_Str

class Safe_Str__Benchmark_Id(Safe_Str):                    # e.g., "A_01__python__nop"
    max_length = 100
```

**`Safe_Str__Benchmark__Section`** - Section identifier
```python
# File: benchmark/schemas/safe_str/Safe_Str__Benchmark__Section.py
from osbot_utils.type_safe.primitives.core.Safe_Str import Safe_Str

class Safe_Str__Benchmark__Section(Safe_Str):              # e.g., "A", "Python"
    max_length = 50
```

**`Safe_Str__Benchmark__Index`** - Index within section
```python
# File: benchmark/schemas/safe_str/Safe_Str__Benchmark__Index.py
from osbot_utils.type_safe.primitives.core.Safe_Str import Safe_Str

class Safe_Str__Benchmark__Index(Safe_Str):                # e.g., "01", "02"
    max_length = 10
```

### 4.2 Enums

**`Enum__Time_Unit`** - Time display unit
```python
# File: benchmark/schemas/enums/Enum__Time_Unit.py
from enum import Enum

class Enum__Time_Unit(Enum):
    NANOSECONDS  = 'ns'
    MICROSECONDS = 'µs'
    MILLISECONDS = 'ms'
    SECONDS      = 's'
```

**`Enum__Hypothesis__Status`** - Hypothesis outcome
```python
# File: benchmark/schemas/enums/Enum__Hypothesis__Status.py
from enum import Enum

class Enum__Hypothesis__Status(Enum):
    SUCCESS      = 'success'                               # Met or exceeded target
    FAILURE      = 'failure'                               # Did not meet target
    INCONCLUSIVE = 'inconclusive'                          # Mixed results
    REGRESSION   = 'regression'                            # Performance got worse
```

### 4.3 Collections

**`Dict__Benchmark_Results`** - Results collection
```python
# File: benchmark/schemas/collections/Dict__Benchmark_Results.py
from osbot_utils.type_safe.shared.Type_Safe__Dict                                            import Type_Safe__Dict
from osbot_utils.testing.performance.benchmark.schemas.safe_str.Safe_Str__Benchmark_Id       import Safe_Str__Benchmark_Id
from osbot_utils.testing.performance.benchmark.schemas.Schema__Perf__Benchmark__Result       import Schema__Perf__Benchmark__Result

class Dict__Benchmark_Results(Type_Safe__Dict):
    expected_key_type   = Safe_Str__Benchmark_Id
    expected_value_type = Schema__Perf__Benchmark__Result
```

**`Dict__Benchmark__Legend`** - Section legend
```python
# File: benchmark/schemas/collections/Dict__Benchmark__Legend.py
from osbot_utils.type_safe.shared.Type_Safe__Dict                                            import Type_Safe__Dict
from osbot_utils.type_safe.primitives.core.Safe_Str                                          import Safe_Str
from osbot_utils.testing.performance.benchmark.schemas.safe_str.Safe_Str__Benchmark__Section import Safe_Str__Benchmark__Section

class Dict__Benchmark__Legend(Type_Safe__Dict):
    expected_key_type   = Safe_Str__Benchmark__Section
    expected_value_type = Safe_Str
```

**`List__Benchmark_Sessions`** - Sessions list for Diff
```python
# File: benchmark/schemas/collections/List__Benchmark_Sessions.py
from osbot_utils.type_safe.shared.Type_Safe__List                                            import Type_Safe__List
from osbot_utils.testing.performance.benchmark.schemas.Schema__Perf__Benchmark__Session      import Schema__Perf__Benchmark__Session

class List__Benchmark_Sessions(Type_Safe__List):
    expected_type = Schema__Perf__Benchmark__Session
```

### 4.4 Result Schemas

**`Schema__Perf__Benchmark__Result`** - Single benchmark result
```python
# File: benchmark/schemas/Schema__Perf__Benchmark__Result.py
from osbot_utils.type_safe.Type_Safe                                                         import Type_Safe
from osbot_utils.type_safe.primitives.core.Safe_Str                                          import Safe_Str
from osbot_utils.type_safe.primitives.core.Safe_UInt                                         import Safe_UInt
from osbot_utils.testing.performance.benchmark.schemas.safe_str.Safe_Str__Benchmark_Id       import Safe_Str__Benchmark_Id
from osbot_utils.testing.performance.benchmark.schemas.safe_str.Safe_Str__Benchmark__Section import Safe_Str__Benchmark__Section
from osbot_utils.testing.performance.benchmark.schemas.safe_str.Safe_Str__Benchmark__Index   import Safe_Str__Benchmark__Index

class Schema__Perf__Benchmark__Result(Type_Safe):          # Single benchmark result (pure data)
    benchmark_id : Safe_Str__Benchmark_Id                  # Full ID: "A_01__python__nop"
    section      : Safe_Str__Benchmark__Section            # Extracted: "A"
    index        : Safe_Str__Benchmark__Index              # Extracted: "01"
    name         : Safe_Str                                # Extracted: "python__nop"
    final_score  : Safe_UInt                               # Normalized score in ns
    raw_score    : Safe_UInt                               # Raw score in ns
```

**`Schema__Perf__Benchmark__Session`** - Full session for serialization
```python
# File: benchmark/schemas/Schema__Perf__Benchmark__Session.py
from osbot_utils.type_safe.Type_Safe                                                         import Type_Safe
from osbot_utils.type_safe.primitives.core.Safe_Str                                          import Safe_Str
from osbot_utils.type_safe.primitives.domains.identifiers.Timestamp_Now                      import Timestamp_Now
from osbot_utils.testing.performance.benchmark.schemas.collections.Dict__Benchmark_Results   import Dict__Benchmark_Results
from osbot_utils.testing.performance.benchmark.schemas.collections.Dict__Benchmark__Legend   import Dict__Benchmark__Legend

class Schema__Perf__Benchmark__Session(Type_Safe):         # Full session (pure data)
    title       : Safe_Str                                 # Session title
    description : Safe_Str                                 # Optional description
    timestamp   : Timestamp_Now                            # When session was run
    results     : Dict__Benchmark_Results                  # All benchmark results
    legend      : Dict__Benchmark__Legend                  # Section descriptions
```

**`Schema__Perf__Hypothesis__Result`** - Hypothesis outcome
```python
# File: benchmark/schemas/Schema__Perf__Hypothesis__Result.py
from osbot_utils.type_safe.Type_Safe                                                          import Type_Safe
from osbot_utils.type_safe.primitives.core.Safe_Str                                           import Safe_Str
from osbot_utils.type_safe.primitives.core.Safe_Float                                         import Safe_Float
from osbot_utils.type_safe.primitives.domains.identifiers.Timestamp_Now                       import Timestamp_Now
from osbot_utils.testing.performance.benchmark.schemas.collections.Dict__Benchmark_Results    import Dict__Benchmark_Results
from osbot_utils.testing.performance.benchmark.schemas.enums.Enum__Hypothesis__Status         import Enum__Hypothesis__Status

class Schema__Perf__Hypothesis__Result(Type_Safe):         # Hypothesis outcome (pure data)
    description        : Safe_Str                          # What we're testing
    target_improvement : Safe_Float                        # e.g., 0.5 for 50%
    actual_improvement : Safe_Float                        # Calculated from results
    before_results     : Dict__Benchmark_Results           # Baseline measurements
    after_results      : Dict__Benchmark_Results           # Optimized measurements
    status             : Enum__Hypothesis__Status          # SUCCESS, FAILURE, etc.
    timestamp          : Timestamp_Now                     # When evaluated
    comments           : Safe_Str                          # Optional notes
```

---

## 5. Class Specifications

### 5.1 `Perf_Benchmark__Timing__Config` (Schema - Pure Data)

```python
# File: benchmark/Perf_Benchmark__Timing__Config.py
from osbot_utils.type_safe.Type_Safe                                                         import Type_Safe
from osbot_utils.type_safe.primitives.core.Safe_Str                                          import Safe_Str
from osbot_utils.testing.performance.benchmark.schemas.collections.Dict__Benchmark__Legend   import Dict__Benchmark__Legend
from osbot_utils.testing.performance.benchmark.schemas.enums.Enum__Time_Unit                 import Enum__Time_Unit

class Perf_Benchmark__Timing__Config(Type_Safe):           # Configuration (pure data)
    title                   : Safe_Str                     # "Type_Safe__Config Performance Baselines"
    description             : Safe_Str                     # Optional subtitle
    output_path             : Safe_Str                     # Base path for output files
    output_prefix           : Safe_Str                     # Filename prefix
    legend                  : Dict__Benchmark__Legend      # Optional: {'A': 'Python', 'B': 'Config'}
    time_unit               : Enum__Time_Unit = Enum__Time_Unit.NANOSECONDS
    print_to_console        : bool            = True       # Print on report generation
    auto_save_on_completion : bool            = False      # Save in stop()/__exit__/tearDownClass
```

### 5.2 `Perf_Benchmark__Timing` (Core Logic - Has Methods)

```python
# File: benchmark/Perf_Benchmark__Timing.py
from typing                                                                                  import Callable, Optional
from osbot_utils.type_safe.Type_Safe                                                         import Type_Safe
from osbot_utils.type_safe.primitives.core.Safe_UInt                                         import Safe_UInt
from osbot_utils.testing.performance.Performance_Measure__Session                            import Perf
from osbot_utils.testing.performance.benchmark.Perf_Benchmark__Timing__Config                import Perf_Benchmark__Timing__Config
from osbot_utils.testing.performance.benchmark.Perf_Benchmark__Timing__Reporter              import Perf_Benchmark__Timing__Reporter
from osbot_utils.testing.performance.benchmark.schemas.collections.Dict__Benchmark_Results   import Dict__Benchmark_Results
from osbot_utils.testing.performance.benchmark.schemas.Schema__Perf__Benchmark__Result       import Schema__Perf__Benchmark__Result
from osbot_utils.testing.performance.benchmark.schemas.safe_str.Safe_Str__Benchmark_Id       import Safe_Str__Benchmark_Id

class Perf_Benchmark__Timing(Type_Safe):
    config  : Perf_Benchmark__Timing__Config               # Configuration
    results : Dict__Benchmark_Results                      # Collected results
    session : Perf                                         # Performance measurement session
    
    # Standard thresholds (nanoseconds)
    time_100_ns  : Safe_UInt =     100
    time_500_ns  : Safe_UInt =     500
    time_1_kns   : Safe_UInt =   1_000
    time_2_kns   : Safe_UInt =   2_000
    time_5_kns   : Safe_UInt =   5_000
    time_10_kns  : Safe_UInt =  10_000
    time_20_kns  : Safe_UInt =  20_000
    time_50_kns  : Safe_UInt =  50_000
    time_100_kns : Safe_UInt = 100_000
    
    # Lifecycle methods
    def start(self) -> 'Perf_Benchmark__Timing': ...       # Initialize session
    def stop(self) -> 'Perf_Benchmark__Timing': ...        # Finalize, optionally save
    
    # Context manager
    def __enter__(self) -> 'Perf_Benchmark__Timing': ...   # Calls start()
    def __exit__(self, *args) -> None: ...                 # Calls stop()
    
    # Core benchmark method
    def benchmark(self, 
                  benchmark_id    : Safe_Str__Benchmark_Id             ,
                  target          : Callable                           ,
                  assert_less_than: Optional[Safe_UInt] = None         ) -> Schema__Perf__Benchmark__Result: ...
    
    # Reporter access
    def reporter(self) -> Perf_Benchmark__Timing__Reporter: ...
    
    # Helpers
    def _parse_benchmark_id(self, benchmark_id: Safe_Str__Benchmark_Id) -> tuple: ...
```

### 5.3 `TestCase__Benchmark__Timing` (TestCase Subclass)

```python
# File: benchmark/TestCase__Benchmark__Timing.py
from typing                                                                                  import Callable, Optional
from unittest                                                                                import TestCase
from osbot_utils.type_safe.primitives.core.Safe_UInt                                         import Safe_UInt
from osbot_utils.testing.performance.benchmark.Perf_Benchmark__Timing                        import Perf_Benchmark__Timing
from osbot_utils.testing.performance.benchmark.Perf_Benchmark__Timing__Config                import Perf_Benchmark__Timing__Config
from osbot_utils.testing.performance.benchmark.schemas.Schema__Perf__Benchmark__Result       import Schema__Perf__Benchmark__Result
from osbot_utils.testing.performance.benchmark.schemas.safe_str.Safe_Str__Benchmark_Id       import Safe_Str__Benchmark_Id

class TestCase__Benchmark__Timing(TestCase):               # For use in test files
    config : Perf_Benchmark__Timing__Config                # Class attribute - override in subclass
    timing : Perf_Benchmark__Timing                        # Set in setUpClass
    
    # Expose thresholds at class level for convenience
    time_100_ns  = 100
    time_500_ns  = 500
    time_1_kns   = 1_000
    time_2_kns   = 2_000
    time_5_kns   = 5_000
    time_10_kns  = 10_000
    time_20_kns  = 20_000
    time_50_kns  = 50_000
    time_100_kns = 100_000
    
    @classmethod
    def setUpClass(cls) -> None:
        cls.timing = Perf_Benchmark__Timing(config=cls.config)
        cls.timing.start()
    
    @classmethod
    def tearDownClass(cls) -> None:
        cls.timing.stop()                                  # Handles auto_save if configured
        if cls.config.print_to_console:
            cls.timing.reporter().print_summary()
    
    # Delegate to timing instance
    def benchmark(self,
                  benchmark_id    : Safe_Str__Benchmark_Id             ,
                  target          : Callable                           ,
                  assert_less_than: Optional[Safe_UInt] = None         ) -> Schema__Perf__Benchmark__Result:
        return self.timing.benchmark(benchmark_id, target, assert_less_than)
    
    # Access reporter
    @classmethod
    def reporter(cls) -> 'Perf_Benchmark__Timing__Reporter':
        return cls.timing.reporter()
```

### 5.4 `Perf_Benchmark__Timing__Reporter` (Has Methods)

```python
# File: benchmark/Perf_Benchmark__Timing__Reporter.py
from osbot_utils.type_safe.Type_Safe                                                         import Type_Safe
from osbot_utils.type_safe.primitives.core.Safe_Str                                          import Safe_Str
from osbot_utils.utils.Print_Table                                                           import Print_Table
from osbot_utils.testing.performance.benchmark.Perf_Benchmark__Timing__Config                import Perf_Benchmark__Timing__Config
from osbot_utils.testing.performance.benchmark.schemas.collections.Dict__Benchmark_Results   import Dict__Benchmark_Results
from osbot_utils.testing.performance.benchmark.schemas.collections.Dict__Benchmark__Legend   import Dict__Benchmark__Legend

class Perf_Benchmark__Timing__Reporter(Type_Safe):
    results : Dict__Benchmark_Results                      # Benchmark results
    config  : Perf_Benchmark__Timing__Config               # Configuration
    
    # Output generation (uses Print_Table internally)
    def build_text(self)     -> Safe_Str: ...              # Formatted text with sections
    def build_json(self)     -> dict: ...                  # Native dict for json.dump
    def build_markdown(self) -> Safe_Str: ...              # Markdown tables
    def build_html(self)     -> Safe_Str: ...              # HTML with JavaScript charts
    
    # File operations
    def save_all(self) -> None: ...                        # Writes .txt, .json, .md, .html
    def save_text(self, filepath: Safe_Str) -> None: ...
    def save_json(self, filepath: Safe_Str) -> None: ...
    def save_markdown(self, filepath: Safe_Str) -> None: ...
    def save_html(self, filepath: Safe_Str) -> None: ...
    
    # Comparison (2 sessions)
    def compare(self, other: 'Perf_Benchmark__Timing__Reporter') -> Safe_Str: ...
    def compare_from_json(self, filepath: Safe_Str) -> Safe_Str: ...
    
    # Helpers
    def detect_sections(self) -> Dict__Benchmark__Legend: ...
    def print_summary(self) -> None: ...
    
    # Internal - uses Print_Table
    def _create_results_table(self) -> Print_Table: ...
    def _create_comparison_table(self, other_results: Dict__Benchmark_Results) -> Print_Table: ...
    def _create_html_chart(self) -> Safe_Str: ...
```

### 5.5 `Perf_Benchmark__Hypothesis` (Hypothesis Testing)

```python
# File: benchmark/Perf_Benchmark__Hypothesis.py
from typing                                                                                   import Callable
from osbot_utils.type_safe.Type_Safe                                                          import Type_Safe
from osbot_utils.type_safe.primitives.core.Safe_Str                                           import Safe_Str
from osbot_utils.type_safe.primitives.core.Safe_Float                                         import Safe_Float
from osbot_utils.testing.performance.benchmark.Perf_Benchmark__Timing                         import Perf_Benchmark__Timing
from osbot_utils.testing.performance.benchmark.schemas.collections.Dict__Benchmark_Results    import Dict__Benchmark_Results
from osbot_utils.testing.performance.benchmark.schemas.Schema__Perf__Hypothesis__Result       import Schema__Perf__Hypothesis__Result

class Perf_Benchmark__Hypothesis(Type_Safe):
    description        : Safe_Str                          # What we're testing
    target_improvement : Safe_Float                        # e.g., 0.5 for 50%
    before_results     : Dict__Benchmark_Results           # Baseline (captured first)
    after_results      : Dict__Benchmark_Results           # Optimized (captured after code changes)
    comments           : Safe_Str                          # Optional notes
    
    # Capture methods
    def run_before(self, 
                   benchmarks: Callable[[Perf_Benchmark__Timing], None]) -> 'Perf_Benchmark__Hypothesis': ...
    
    def run_after(self, 
                  benchmarks: Callable[[Perf_Benchmark__Timing], None]) -> 'Perf_Benchmark__Hypothesis': ...
    
    # Evaluation
    def evaluate(self) -> Schema__Perf__Hypothesis__Result: ...
    
    # Persistence
    def save(self, filepath: Safe_Str) -> None: ...
    
    @classmethod
    def load(cls, filepath: Safe_Str) -> 'Perf_Benchmark__Hypothesis': ...
    
    # Reporting
    def print_result(self) -> None: ...
```

### 5.6 `Perf_Benchmark__Diff` (Multi-Session Comparison)

```python
# File: benchmark/Perf_Benchmark__Diff.py
from osbot_utils.type_safe.Type_Safe                                                          import Type_Safe
from osbot_utils.type_safe.primitives.core.Safe_Str                                           import Safe_Str
from osbot_utils.testing.performance.benchmark.schemas.collections.List__Benchmark_Sessions   import List__Benchmark_Sessions
from osbot_utils.testing.performance.benchmark.schemas.Schema__Perf__Benchmark__Session       import Schema__Perf__Benchmark__Session
from osbot_utils.utils.Print_Table                                                            import Print_Table

class Perf_Benchmark__Diff(Type_Safe):
    sessions : List__Benchmark_Sessions                    # Loaded sessions
    
    # Loading
    def load_session(self, filepath: Safe_Str) -> 'Perf_Benchmark__Diff': ...
    def load_folder(self, folder_path: Safe_Str) -> 'Perf_Benchmark__Diff': ...
    
    # Comparison
    def compare_two(self, 
                    session_a: Schema__Perf__Benchmark__Session = None,
                    session_b: Schema__Perf__Benchmark__Session = None) -> Safe_Str: ...
    
    def compare_all(self) -> Safe_Str: ...
    
    # Output
    def build_evolution_text(self) -> Safe_Str: ...
    def build_evolution_html(self) -> Safe_Str: ...        # With Chart.js graphs
    def build_statistics(self) -> Safe_Str: ...
    
    # File operations
    def save_comparison(self, filepath: Safe_Str) -> None: ...
    def save_evolution_html(self, filepath: Safe_Str) -> None: ...
    
    # Helpers
    def _create_evolution_table(self) -> Print_Table: ...
    def _create_chart_js_data(self) -> Safe_Str: ...
```

---

## 6. Key Method Implementations

### 6.1 `Perf_Benchmark__Timing.benchmark()`

```python
def benchmark(self, benchmark_id, target, assert_less_than=None):
    self.session.measure__quick(target)
    perf_result = self.session.result
    
    # Extract section and index from ID
    section, index, name = self._parse_benchmark_id(benchmark_id)
    
    result = Schema__Perf__Benchmark__Result(
        benchmark_id = benchmark_id                                               ,
        section      = section                                                    ,
        index        = index                                                      ,
        name         = name                                                       ,
        final_score  = Safe_UInt(int(perf_result.final_score))                    ,
        raw_score    = Safe_UInt(int(perf_result.raw_score  ))                    )
    
    self.results[benchmark_id] = result
    
    if assert_less_than:
        self.session.assert_time__less_than(int(assert_less_than))
    
    return result

def _parse_benchmark_id(self, benchmark_id):
    # "A_01__python__nop" → ("A", "01", "python__nop")
    parts = str(benchmark_id).split('_', 2)
    if len(parts) >= 3:
        return (Safe_Str__Benchmark__Section(parts[0]), 
                Safe_Str__Benchmark__Index(parts[1]), 
                Safe_Str(parts[2].lstrip('_')))
    return (Safe_Str__Benchmark__Section(''), 
            Safe_Str__Benchmark__Index(''), 
            Safe_Str(str(benchmark_id)))
```

### 6.2 `Perf_Benchmark__Timing.start()` / `stop()`

```python
def start(self):
    self.session = Perf(assert_enabled=True)
    self.results = Dict__Benchmark_Results()
    return self

def stop(self):
    if self.config.auto_save_on_completion and self.config.output_path:
        self.reporter().save_all()
    return self
```

### 6.3 Context Manager

```python
def __enter__(self):
    return self.start()

def __exit__(self, exc_type, exc_val, exc_tb):
    self.stop()
    return False
```

### 6.4 Reporter Using Print_Table

```python
def _create_results_table(self) -> Print_Table:
    table = Print_Table()
    table.set_title(str(self.config.title))
    table.add_headers('ID', 'Benchmark', 'Score', 'Raw')
    
    # Group by section
    current_section = None
    
    for benchmark_id in sorted(self.results.keys()):
        result  = self.results[benchmark_id]
        section = str(result.section)
        
        # Add section separator row if section changed
        if section != current_section:
            if current_section is not None:
                table.add_row(['─' * 6, '─' * 30, '─' * 12, '─' * 12])
            current_section = section
        
        # Format score based on time_unit
        score_str = self._format_time(result.final_score)
        raw_str   = self._format_time(result.raw_score)
        
        table.add_row([
            f"{result.section}_{result.index}"            ,
            str(result.name)                              ,
            score_str                                     ,
            raw_str                                       ])
    
    table.set_footer(f"Total: {len(self.results)} benchmarks")
    return table

def _format_time(self, ns_value: Safe_UInt) -> Safe_Str:
    value = int(ns_value)
    unit  = self.config.time_unit
    
    if unit == Enum__Time_Unit.NANOSECONDS:
        return Safe_Str(f"{value:,} ns")
    elif unit == Enum__Time_Unit.MICROSECONDS:
        return Safe_Str(f"{value / 1_000:,.3f} µs")
    elif unit == Enum__Time_Unit.MILLISECONDS:
        return Safe_Str(f"{value / 1_000_000:,.3f} ms")
    elif unit == Enum__Time_Unit.SECONDS:
        return Safe_Str(f"{value / 1_000_000_000:,.6f} s")
    return Safe_Str(f"{value:,} ns")

def print_summary(self):
    table = self._create_results_table()
    table.print()
```

### 6.5 HTML Chart Generation

```python
def build_html(self) -> Safe_Str:
    # Generate standalone HTML with Chart.js
    chart_data = self._create_chart_js_data()
    
    html = f'''<!DOCTYPE html>
<html>
<head>
    <title>{self.config.title}</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
        body {{ font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; margin: 20px; }}
        .chart-container {{ max-width: 800px; margin: 20px auto; }}
        table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f4f4f4; }}
    </style>
</head>
<body>
    <h1>{self.config.title}</h1>
    <div class="chart-container">
        <canvas id="benchmarkChart"></canvas>
    </div>
    {self._create_html_table()}
    <script>
        {chart_data}
    </script>
</body>
</html>'''
    return Safe_Str(html)
```

### 6.6 Hypothesis Evaluation

```python
def evaluate(self) -> Schema__Perf__Hypothesis__Result:
    if not self.before_results or not self.after_results:
        raise ValueError("Must run both before and after benchmarks")
    
    # Calculate improvements per benchmark
    improvements = []
    for benchmark_id in self.before_results.keys():
        if benchmark_id in self.after_results:
            before = self.before_results[benchmark_id].final_score
            after  = self.after_results[benchmark_id].final_score
            if before > 0:
                improvement = (before - after) / before
                improvements.append(improvement)
    
    # Calculate average improvement
    actual_improvement = sum(improvements) / len(improvements) if improvements else 0
    
    # Determine status
    if actual_improvement >= self.target_improvement:
        status = Enum__Hypothesis__Status.SUCCESS
    elif actual_improvement < 0:
        status = Enum__Hypothesis__Status.REGRESSION
    elif actual_improvement > 0:
        status = Enum__Hypothesis__Status.FAILURE
    else:
        status = Enum__Hypothesis__Status.INCONCLUSIVE
    
    return Schema__Perf__Hypothesis__Result(
        description        = self.description                                     ,
        target_improvement = self.target_improvement                              ,
        actual_improvement = Safe_Float(actual_improvement)                       ,
        before_results     = self.before_results                                  ,
        after_results      = self.after_results                                   ,
        status             = status                                               ,
        comments           = self.comments                                        )
```

### 6.7 Diff Load Folder

```python
def load_folder(self, folder_path: Safe_Str) -> 'Perf_Benchmark__Diff':
    from osbot_utils.utils.Files import files_list, file_extension
    from osbot_utils.utils.Json  import json_load_file
    
    folder = str(folder_path)
    json_files = [f for f in files_list(folder) if file_extension(f) == '.json']
    
    for filepath in sorted(json_files):                    # Sort for consistent ordering
        session_data = json_load_file(filepath)
        session = Schema__Perf__Benchmark__Session.from_json(session_data)
        self.sessions.append(session)
    
    return self
```

---

## 7. Output Examples

### 7.1 Text Output (via Print_Table)
```
┌────────────────────────────────────────────────────────────────────────────────────┐
│ Type_Safe__Config Performance Baselines                                            │
├────────────────────────────────────────────────────────────────────────────────────┤
│ ID     │ Benchmark                      │ Score        │ Raw                       │
├────────────────────────────────────────────────────────────────────────────────────┤
│ A_01   │ python__nop                    │      100 ns  │       82 ns               │
│ A_02   │ python__var_assignment         │      100 ns  │       82 ns               │
├────────────────────────────────────────────────────────────────────────────────────┤
│ B_01   │ config_creation__default       │      300 ns  │      281 ns               │
├────────────────────────────────────────────────────────────────────────────────────┤
│ Total: 52 benchmarks                                                               │
└────────────────────────────────────────────────────────────────────────────────────┘
```

### 7.2 Comparison Output (Two Sessions)
```
┌────────────────────────────────────────────────────────────────────────────────────┐
│ Comparison: Stats A vs Stats B                                                     │
├────────────────────────────────────────────────────────────────────────────────────┤
│ Benchmark                      │ Before      │ After       │ Change               │
├────────────────────────────────────────────────────────────────────────────────────┤
│ D_01__type_safe__empty         │     800 ns  │     300 ns  │ -62.5% ▼             │
│ D_02__type_safe__with_prims    │   5,000 ns  │     500 ns  │ -90.0% ▼             │
│ D_03__type_safe__with_nested   │  20,000 ns  │   1,000 ns  │ -95.0% ▼             │
├────────────────────────────────────────────────────────────────────────────────────┤
│ Average improvement: 82.5%                                                         │
└────────────────────────────────────────────────────────────────────────────────────┘
```

### 7.3 Multi-Session Evolution
```
┌────────────────────────────────────────────────────────────────────────────────────┐
│ Performance Evolution: 4 sessions                                                  │
├────────────────────────────────────────────────────────────────────────────────────┤
│ Benchmark             │ Jan 1    │ Jan 3    │ Jan 5    │ Jan 6    │ Trend         │
├────────────────────────────────────────────────────────────────────────────────────┤
│ type_safe__empty      │   800 ns │   600 ns │   400 ns │   300 ns │ ▼▼▼ -62.5%   │
│ type_safe__prims      │ 5,000 ns │ 4,000 ns │ 2,000 ns │   500 ns │ ▼▼▼ -90.0%   │
│ type_safe__nested     │20,000 ns │15,000 ns │ 5,000 ns │ 1,000 ns │ ▼▼▼ -95.0%   │
├────────────────────────────────────────────────────────────────────────────────────┤
│ Overall trend: Steady improvement across all sessions                              │
└────────────────────────────────────────────────────────────────────────────────────┘
```

### 7.4 Hypothesis Result
```
┌────────────────────────────────────────────────────────────────────────────────────┐
│ HYPOTHESIS RESULT                                                                  │
├────────────────────────────────────────────────────────────────────────────────────┤
│ Description: "skip_setattr reduces Type_Safe creation time by 50%"                 │
│ Target: 50% improvement                                                            │
├────────────────────────────────────────────────────────────────────────────────────┤
│ Benchmark                      │ Before      │ After       │ Change               │
├────────────────────────────────────────────────────────────────────────────────────┤
│ type_safe__empty               │     800 ns  │     300 ns  │ -62.5% ▼             │
│ type_safe__primitives          │   5,000 ns  │     500 ns  │ -90.0% ▼             │
├────────────────────────────────────────────────────────────────────────────────────┤
│ Actual improvement: 76.25%                                                         │
│                                                                                    │
│ ╔════════════════════════════════════════════════════════════════════════════════╗│
│ ║  STATUS: ✓ SUCCESS  (76.25% > 50% target)                                      ║│
│ ╚════════════════════════════════════════════════════════════════════════════════╝│
└────────────────────────────────────────────────────────────────────────────────────┘
```

---

## 8. Usage Examples

### 8.1 Basic Test File

```python
from osbot_utils.testing.performance.benchmark.TestCase__Benchmark__Timing    import TestCase__Benchmark__Timing
from osbot_utils.testing.performance.benchmark.Perf_Benchmark__Timing__Config import Perf_Benchmark__Timing__Config

class Empty_Class:
    pass

class test_perf__My_Benchmarks(TestCase__Benchmark__Timing):
    
    config = Perf_Benchmark__Timing__Config(title = "My Performance Baselines")
    
    def test__A_01__python__nop(self):
        self.benchmark('A_01__python__nop', lambda: None)
    
    def test__A_02__python__class_empty(self):
        self.benchmark('A_02__python__class_empty', lambda: Empty_Class())
    
    def test__B_01__something_complex(self):
        def do_complex_thing():
            return [i * 2 for i in range(100)]
        
        self.benchmark('B_01__something_complex', do_complex_thing,
                       assert_less_than=self.time_10_kns)
```

### 8.2 With Auto-Save

```python
class test_perf__My_Benchmarks(TestCase__Benchmark__Timing):
    
    config = Perf_Benchmark__Timing__Config(
        title                   = "My Performance Baselines"    ,
        output_path             = "tests/performance/stats"     ,
        output_prefix           = "my_benchmarks"               ,
        auto_save_on_completion = True                          )  # Saves in tearDownClass
```

### 8.3 Using Context Manager

```python
from osbot_utils.testing.performance.benchmark.Perf_Benchmark__Timing        import Perf_Benchmark__Timing
from osbot_utils.testing.performance.benchmark.Perf_Benchmark__Timing__Config import Perf_Benchmark__Timing__Config

config = Perf_Benchmark__Timing__Config(
    title                   = "Ad-hoc Benchmarks"              ,
    output_path             = "/tmp/benchmarks"                ,
    auto_save_on_completion = True                             )

with Perf_Benchmark__Timing(config=config) as timing:
    timing.benchmark('A_01__test_one', lambda: some_operation())
    timing.benchmark('A_02__test_two', lambda: other_operation())
# Auto-saves on context exit
```

### 8.4 Hypothesis Testing

```python
from osbot_utils.testing.performance.benchmark.Perf_Benchmark__Hypothesis import Perf_Benchmark__Hypothesis

# Define the hypothesis
hypothesis = Perf_Benchmark__Hypothesis(
    description        = "skip_setattr reduces Type_Safe creation by 50%"         ,
    target_improvement = 0.5                                                       )

# Define benchmarks to run
def run_benchmarks(timing):
    timing.benchmark('type_safe__empty', lambda: Type_Safe())
    timing.benchmark('type_safe__prims', lambda: TS__With_Primitives())

# STEP 1: Capture baseline (BEFORE making any code changes!)
hypothesis.run_before(run_benchmarks)

# STEP 2: Make code changes
# ... modify Type_Safe.__init__ ...

# STEP 3: Capture after
hypothesis.run_after(run_benchmarks)

# STEP 4: Evaluate
result = hypothesis.evaluate()
hypothesis.print_result()

# STEP 5: Save
hypothesis.save('/benchmarks/hypotheses/skip_setattr.json')
```

### 8.5 Multi-Session Comparison (Diff)

```python
from osbot_utils.testing.performance.benchmark.Perf_Benchmark__Diff import Perf_Benchmark__Diff

# Load sessions from folder
diff = Perf_Benchmark__Diff()
diff.load_folder('/benchmarks/type_safe/')

# Generate evolution report
diff.compare_all()

# Generate HTML with charts
diff.save_evolution_html('/reports/type_safe_evolution.html')
```

---

## 9. Implementation Order

| Step | Component | Files | Priority |
|------|-----------|-------|----------|
| 1 | Custom primitives | `Safe_Str__Benchmark_Id.py`, `Safe_Str__Benchmark__Section.py`, `Safe_Str__Benchmark__Index.py` | High |
| 2 | Enums | `Enum__Time_Unit.py`, `Enum__Hypothesis__Status.py` | High |
| 3 | Result schemas | `Schema__Perf__Benchmark__Result.py`, `Schema__Perf__Benchmark__Session.py` | High |
| 4 | Collections | `Dict__Benchmark_Results.py`, `Dict__Benchmark__Legend.py`, `List__Benchmark_Sessions.py` | High |
| 5 | Config | `Perf_Benchmark__Timing__Config.py` | High |
| 6 | Reporter | `Perf_Benchmark__Timing__Reporter.py` | High |
| 7 | Core timing | `Perf_Benchmark__Timing.py` | High |
| 8 | TestCase | `TestCase__Benchmark__Timing.py` | High |
| 9 | Tests (core) | Unit tests for steps 1-8 | High |
| 10 | HTML export | Add `build_html()` to Reporter | Medium |
| 11 | Hypothesis schema | `Schema__Perf__Hypothesis__Result.py` | Medium |
| 12 | Hypothesis class | `Perf_Benchmark__Hypothesis.py` | Medium |
| 13 | Diff class | `Perf_Benchmark__Diff.py` | Medium |
| 14 | Tests (extended) | Unit tests for steps 10-13 | Medium |
| 15 | Refactor existing | `test_perf__Type_Safe__Config.py` | Medium |

---

## 10. Summary Checklist

When implementing:

**Core Infrastructure:**
- [ ] All schemas inherit from `Type_Safe` and are pure data (no methods)
- [ ] All primitives go in `benchmark/schemas/safe_str/`
- [ ] All collections go in `benchmark/schemas/collections/`
- [ ] Each schema class is in its own file
- [ ] No raw `str`, `int`, `dict` - use Safe_* and Type_Safe collections
- [ ] `Enum__Time_Unit` controls display formatting
- [ ] `Perf_Benchmark__Timing` has `start()`/`stop()` methods
- [ ] `Perf_Benchmark__Timing` supports context manager
- [ ] `TestCase__Benchmark__Timing` extends `TestCase` for test files
- [ ] `auto_save_on_completion` triggers save in `stop()`/`__exit__`/`tearDownClass`
- [ ] Reporter uses `Print_Table` for formatted output
- [ ] Follow Python formatting guide (aligned assignments, right-aligned comments)

**Extended Features:**
- [ ] Reporter generates `.txt`, `.json`, `.md`, `.html` outputs
- [ ] HTML includes Chart.js for visualization
- [ ] `Perf_Benchmark__Hypothesis` captures before/after with evaluation
- [ ] Hypothesis workflow enforces baseline-first pattern
- [ ] `Perf_Benchmark__Diff` loads sessions from files
- [ ] Diff supports loading entire folders of JSON files
- [ ] Multi-session evolution tracking with trend analysis
- [ ] All outputs can be reconstructed from saved JSON files

---

*Document: Implementation Brief - Perf_Benchmark__Timing Infrastructure*  
*Version: v3.66.0*  
*Date: 6th January 2026*
